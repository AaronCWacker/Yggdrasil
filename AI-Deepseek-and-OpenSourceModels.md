Overview of distilled open source MoE model Deepseek R1.  


Open source and best techniques together made it possible and arXiv papers showed it was coming.  It is very similar to Mistral/Mixtral backstory.  Huggingface open source models showed how more narrow content MoE models perform better per domain.  Also that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models that are faster and more performant for focused experts in MoE.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques as well haven't been fully recombined yet where each have an edge.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets big models in US are already their best since we know the content flaws.  We have a long way to go making models better and open by intercombining techniques and models and I feel many companies that their content is key to their problem and it wont always be general performance that leads for any given topic..



Open source and best techniques together made it possible and arXiv papers showed it was coming.  Very similar to Mistral backstory.

Huggingface open source models showed 2021-current how more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques haven't been fully recombined yet.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets on big models in US are already their best.  We have a long way to go intercombining techniques.

We have a long way to go on model improvement and speed increase while cost reduce by focusing on what matters.

Open source and best techniques together made it possible and arXiv papers showed it was coming.  Very similar to Mistral backstory.  Huggingface open source models showed 2021-current how more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models that are faster and more performant for focused experts in MoE.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques as well haven't been fully recombined yet.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets on big models in US are already their best.  We have a long way to go making models better and open by intercombining techniques and models.



