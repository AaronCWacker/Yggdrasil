Overview of distilled open source MoE model Deepseek R1.  

Open source and best techniques together made it possible and arXiv papers showed it was coming.  Very similar to Mistral backstory.

Huggingface open source models showed 2021-current how more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques haven't been fully recombined yet.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets on big models in US are already their best.  We have a long way to go intercombining techniques.

We have a long way to go on model improvement and speed increase while cost reduce by focusing on what matters.

Open source and best techniques together made it possible and arXiv papers showed it was coming.  Very similar to Mistral backstory.  Huggingface open source models showed 2021-current how more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models that are faster and more performant for focused experts in MoE.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques as well haven't been fully recombined yet.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets on big models in US are already their best.  We have a long way to go making models better and open by intercombining techniques and models.



