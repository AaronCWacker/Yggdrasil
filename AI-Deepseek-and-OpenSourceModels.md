Overview of distilled open source MoE model Deepseek R1.  


---

![image](https://github.com/user-attachments/assets/d198061c-3145-4672-bb82-d41c53fa862f)
![image](https://github.com/user-attachments/assets/a2e1d95c-3491-41b8-a4c4-1df5833e8347)
![image](https://github.com/user-attachments/assets/61ecb362-5fa4-4c7b-afdb-e932708a9632)
![image](https://github.com/user-attachments/assets/073c60c6-3989-4933-b68b-29d67a6a4486)


1. Open source and best techniques together made it possible and arXiv papers showed it was coming.  
2. It is very similar to Mistral/Mixtral backstory.  Distill experts.
3. Huggingface open source models showed how more narrow content MoE models perform better per domain.  
4. We shouldn't over invest or over organize in latest singular closed models.
5. We can use even new closed models to build new models that are faster and more performant for focused experts in MoE. 
6. The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques on arXiv haven't been fully recombined yet where each have an edge. 
7. Input datasets matter, every input dataset is different. 
8. We should not assume our 50+ datasets big models in US are already their best since we know the content flaws. 
9. We have a long way to go making models better and open by intercombining techniques and models 
10. Any company with their content is key to their problem.  It wont always be general performance that leads for any given topic.

Open source and best techniques together made it possible and arXiv papers showed it was coming.  Very similar to Mistral backstory.  Huggingface open source models showed 2021-current how more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models because we can use even closed models to build new models that are faster and more performant for focused experts in MoE.  The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques as well haven't been fully recombined yet.  Input datasets matter, every input dataset is different.  We should not assume our 50+ datasets on big models in US are already their best.  We have a long way to go making models better and open by intercombining techniques and models.



