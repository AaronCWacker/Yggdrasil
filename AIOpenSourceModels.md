Well met, thanks for good overview on distilled open source Deepseek R1.  

Open source and best techniques together made it possible and arXiv papers showed it was coming.  

Huggingface open source models as well in more narrow content MoE focus prove since 2021 that we shouldn't over invest or over organize in latest singular closed models.

The patterns of Self Reward, DeepRL, CoT, MoE, Distillation, and many other SOTA techniques haven't been fully recombined yet.

We have a long way to go on model improvement and speed increase while cost reduce by focusing on what matters.  Great coverage - subscribed!

