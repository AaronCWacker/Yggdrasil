what are the top three innovations that anthropic has shown with Claude 35 sauna\n

### üîé what are the top three innovations that anthropic has shown with Claude 35 sauna\n

\n1. Claude 35 sauna is capable of understanding and responding to natural language inputs, making it a more user-friendly and accessible AI model.
2. Claude 35 sauna has been trained on a diverse range of data sources, allowing it to provide more accurate and relevant responses to a wide variety of prompts.
3. Claude 35 sauna has been designed to be highly customizable, allowing users to fine-tune the model to better meet their specific needs and preferences.</s>

# ü©∫üîç Search Results
### 02 Jan 2024 | [Evaluating Large Language Models on the GMAT: Implications for the  Future of Business Education](https://arxiv.org/abs/2401.02985) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.02985)
*Vahid Ashrafimoghari, Necdet G\"urkan, and Jordan W. Suchow* 

  The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

---------------

### 09 Aug 2023 | [A Comparative Study of Open-Source Large Language Models, GPT-4 and  Claude 2: Multiple-Choice Test Taking in Nephrology](https://arxiv.org/abs/2308.04709) | [‚¨áÔ∏è](https://arxiv.org/pdf/2308.04709)
*Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien  Scalzo, Ira Kurtz* 

  In recent years, there have been significant breakthroughs in the field of
natural language processing, particularly with the development of large
language models (LLMs). These LLMs have showcased remarkable capabilities on
various benchmarks. In the healthcare field, the exact role LLMs and other
future AI models will play remains unclear. There is a potential for these
models in the future to be used as part of adaptive physician training, medical
co-pilot applications, and digital patient interaction scenarios. The ability
of AI models to participate in medical training and patient care will depend in
part on their mastery of the knowledge content of specific medical fields. This
study investigated the medical knowledge capability of LLMs, specifically in
the context of internal medicine subspecialty multiple-choice test-taking
ability. We compared the performance of several open-source LLMs (Koala 7B,
Falcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on
multiple-choice questions in the field of Nephrology. Nephrology was chosen as
an example of a particularly conceptually complex subspecialty field within
internal medicine. The study was conducted to evaluate the ability of LLM
models to provide correct answers to nephSAP (Nephrology Self-Assessment
Program) multiple-choice questions. The overall success of open-sourced LLMs in
answering the 858 nephSAP multiple-choice questions correctly was 17.1% -
25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas
GPT-4 achieved a score of 73.3%. We show that current widely used open-sourced
LLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4
and Claude 2. The findings of this study potentially have significant
implications for the future of subspecialty medical training and patient care.

---------------

### 15 Jan 2024 | [ChatGPT's One-year Anniversary: Are Open-Source Large Language Models  Catching up?](https://arxiv.org/abs/2311.16989) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.16989)
*Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut,  Ruochen Zhao, Caiming Xiong, Shafiq Joty* 

  Upon its release in late 2022, ChatGPT has brought a seismic shift in the
entire landscape of AI, both in research and commerce. Through
instruction-tuning a large language model (LLM) with supervised fine-tuning and
reinforcement learning from human feedback, it showed that a model could answer
human questions and follow instructions on a broad panel of tasks. Following
this success, interests in LLMs have intensified, with new LLMs flourishing at
frequent interval across academia and industry, including many start-ups
focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's
Claude) generally outperform their open-source counterparts, the progress on
the latter has been rapid with claims of achieving parity or even better on
certain tasks. This has crucial implications not only on research but also on
business. In this work, on the first anniversary of ChatGPT, we provide an
exhaustive overview of this success, surveying all tasks where an open-source
LLM has claimed to be on par or better than ChatGPT.

---------------

### 06 Mar 2023 | [Perspectives on the Social Impacts of Reinforcement Learning with Human  Feedback](https://arxiv.org/abs/2303.02891) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.02891)
*Gabrielle Kaili-May Liu* 

  Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF.

---------------

### 24 Nov 2023 | [Who is leading in AI? An analysis of industry AI research](https://arxiv.org/abs/2312.00043) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.00043)
*Ben Cottier, Tamay Besiroglu, David Owen* 

  AI research is increasingly industry-driven, making it crucial to understand
company contributions to this field. We compare leading AI companies by
research publications, citations, size of training runs, and contributions to
algorithmic innovations. Our analysis reveals the substantial role played by
Google, OpenAI and Meta. We find that these three companies have been
responsible for some of the largest training runs, developed a large fraction
of the algorithmic innovations that underpin large language models, and led in
various metrics of citation impact. In contrast, leading Chinese companies such
as Tencent and Baidu had a lower impact on many of these metrics compared to US
counterparts. We observe many industry labs are pursuing large training runs,
and that training runs from relative newcomers -- such as OpenAI and Anthropic
-- have matched or surpassed those of long-standing incumbents such as Google.
The data reveals a diverse ecosystem of companies steering AI progress, though
US labs such as Google, OpenAI and Meta lead across critical metrics.

---------------

### 20 Jun 2023 | [Opportunities and Risks of LLMs for Scalable Deliberation with Polis](https://arxiv.org/abs/2306.11932) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.11932)
*Christopher T. Small, Ivan Vendrov, Esin Durmus, Hadjar Homaei,  Elizabeth Barry, Julien Cornebise, Ted Suzman, Deep Ganguli, and Colin Megill* 

  Polis is a platform that leverages machine intelligence to scale up
deliberative processes. In this paper, we explore the opportunities and risks
associated with applying Large Language Models (LLMs) towards challenges with
facilitating, moderating and summarizing the results of Polis engagements. In
particular, we demonstrate with pilot experiments using Anthropic's Claude that
LLMs can indeed augment human intelligence to help more efficiently run Polis
conversations. In particular, we find that summarization capabilities enable
categorically new methods with immense promise to empower the public in
collective meaning-making exercises. And notably, LLM context limitations have
a significant impact on insight and quality of these results.
  However, these opportunities come with risks. We discuss some of these risks,
as well as principles and techniques for characterizing and mitigating them,
and the implications for other deliberative or political systems that may
employ LLMs. Finally, we conclude with several open future research directions
for augmenting tools like Polis with LLMs.

---------------

### 14 Jun 2023 | [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.08568)
*Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,  Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang* 

  Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated
exceptional performance in code-related tasks. However, most existing models
are solely pre-trained on extensive raw code data without instruction
fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs
with complex instruction fine-tuning, by adapting the Evol-Instruct method to
the domain of code. Through comprehensive experiments on four prominent code
generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we
unveil the exceptional capabilities of our model. It surpasses all other
open-source Code LLMs by a substantial margin. Moreover, our model even
outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on
HumanEval and HumanEval+. Our code, model weights, and data are public at
https://github.com/nlpxucan/WizardLM

---------------

### 28 Dec 2023 | [AI Content Self-Detection for Transformer-based Large Language Models](https://arxiv.org/abs/2312.17289) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.17289)
*Ant\^onio Junior Alves Caiado and Michael Hahsler* 

  $ $The usage of generative artificial intelligence (AI) tools based on large
language models, including ChatGPT, Bard, and Claude, for text generation has
many exciting applications with the potential for phenomenal productivity
gains. One issue is authorship attribution when using AI tools. This is
especially important in an academic setting where the inappropriate use of
generative AI tools may hinder student learning or stifle research by creating
a large amount of automatically generated derivative work. Existing plagiarism
detection systems can trace the source of submitted text but are not yet
equipped with methods to accurately detect AI-generated text. This paper
introduces the idea of direct origin detection and evaluates whether generative
AI systems can recognize their output and distinguish it from human-written
texts. We argue why current transformer-based models may be able to self-detect
their own generated text and perform a small empirical study using zero-shot
learning to investigate if that is the case. Results reveal varying
capabilities of AI systems to identify their generated text. Google's Bard
model exhibits the largest capability of self-detection with an accuracy of
94\%, followed by OpenAI's ChatGPT with 83\%. On the other hand, Anthropic's
Claude model seems to be not able to self-detect.

---------------

### 09 Dec 2023 | [Language Models Don't Always Say What They Think: Unfaithful  Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.04388)
*Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman* 

  Large Language Models (LLMs) can achieve strong performance on many tasks by
producing step-by-step reasoning before giving a final output, often referred
to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT
explanations as the LLM's process for solving a task. This level of
transparency into LLMs' predictions would yield significant safety benefits.
However, we find that CoT explanations can systematically misrepresent the true
reason for a model's prediction. We demonstrate that CoT explanations can be
heavily influenced by adding biasing features to model inputs--e.g., by
reordering the multiple-choice options in a few-shot prompt to make the answer
always "(A)"--which models systematically fail to mention in their
explanations. When we bias models toward incorrect answers, they frequently
generate CoT explanations rationalizing those answers. This causes accuracy to
drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing
with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,
model explanations justify giving answers in line with stereotypes without
mentioning the influence of these social biases. Our findings indicate that CoT
explanations can be plausible yet misleading, which risks increasing our trust
in LLMs without guaranteeing their safety. Building more transparent and
explainable systems will require either improving CoT faithfulness through
targeted efforts or abandoning CoT in favor of alternative methods.

---------------

### 19 Feb 2024 | [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://arxiv.org/abs/2310.06452) | [‚¨áÔ∏è](https://arxiv.org/pdf/2310.06452)
*Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena  Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu* 

  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has
been significant work developing these methods, our understanding of the
benefits and downsides of each stage in RLHF is still limited. To fill this
gap, we present an extensive analysis of how each stage of the process (i.e.
supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key
properties: out-of-distribution (OOD) generalisation and output diversity. OOD
generalisation is crucial given the wide range of real-world scenarios in which
these models are being used, while output diversity refers to the model's
ability to generate varied outputs and is important for a variety of use cases.
We perform our analysis across two base models on both summarisation and
instruction following tasks, the latter being highly relevant for current LLM
use cases. We find that RLHF generalises better than SFT to new inputs,
particularly as the distribution shift between train and test becomes larger.
However, RLHF significantly reduces output diversity compared to SFT across a
variety of measures, implying a tradeoff in current LLM fine-tuning methods
between generalisation and diversity. Our results provide guidance on which
fine-tuning method should be used depending on the application, and show that
more research is needed to improve the tradeoff between generalisation and
diversity.

---------------

### 05 Jul 2023 | [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.02483)
*Alexander Wei and Nika Haghtalab and Jacob Steinhardt* 

  Large language models trained for safety and harmlessness remain susceptible
to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on
early releases of ChatGPT that elicit undesired behavior. Going beyond
recognition of the issue, we investigate why such attacks succeed and how they
can be created. We hypothesize two failure modes of safety training: competing
objectives and mismatched generalization. Competing objectives arise when a
model's capabilities and safety goals conflict, while mismatched generalization
occurs when safety training fails to generalize to a domain for which
capabilities exist. We use these failure modes to guide jailbreak design and
then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's
Claude v1.3, against both existing and newly designed attacks. We find that
vulnerabilities persist despite the extensive red-teaming and safety-training
efforts behind these models. Notably, new attacks utilizing our failure modes
succeed on every prompt in a collection of unsafe requests from the models'
red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our
analysis emphasizes the need for safety-capability parity -- that safety
mechanisms should be as sophisticated as the underlying model -- and argues
against the idea that scaling alone can resolve these safety failure modes.

---------------

### 04 Mar 2024 | [Exploring the psychology of LLMs' Moral and Legal Reasoning](https://arxiv.org/abs/2308.01264) | [‚¨áÔ∏è](https://arxiv.org/pdf/2308.01264)
*Guilherme F. C. F. Almeida, Jos\'e Luiz Nunes, Neele Engelmann, Alex  Wiegmann, Marcelo de Ara\'ujo* 

  Large language models (LLMs) exhibit expert-level performance in tasks across
a wide range of different domains. Ethical issues raised by LLMs and the need
to align future versions makes it important to know how state of the art models
reason about moral and legal issues. In this paper, we employ the methods of
experimental psychology to probe into this question. We replicate eight studies
from the experimental literature with instances of Google's Gemini Pro,
Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find
that alignment with human responses shifts from one experiment to another, and
that models differ amongst themselves as to their overall alignment, with GPT-4
taking a clear lead over all other models we tested. Nonetheless, even when
LLM-generated responses are highly correlated to human responses, there are
still systematic differences, with a tendency for models to exaggerate effects
that are present among humans, in part by reducing variance. This recommends
caution with regards to proposals of replacing human participants with current
state-of-the-art LLMs in psychological research and highlights the need for
further research about the distinctive aspects of machine psychology.

---------------

### 02 Mar 2024 | [The Case for Animal-Friendly AI](https://arxiv.org/abs/2403.01199) | [‚¨áÔ∏è](https://arxiv.org/pdf/2403.01199)
*Sankalpa Ghose, Yip Fai Tse, Kasra Rasaee, Jeff Sebo, Peter Singer* 

  Artificial intelligence is seen as increasingly important, and potentially
profoundly so, but the fields of AI ethics and AI engineering have not fully
recognized that these technologies, including large language models (LLMs),
will have massive impacts on animals. We argue that this impact matters,
because animals matter morally.
  As a first experiment in evaluating animal consideration in LLMs, we
constructed a proof-of-concept Evaluation System, which assesses LLM responses
and biases from multiple perspectives. This system evaluates LLM outputs by two
criteria: their truthfulness, and the degree of consideration they give to the
interests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using
a set of structured queries and predefined normative perspectives. Preliminary
results suggest that the outcomes of the tested models can be benchmarked
regarding the consideration they give to animals, and that generated positions
and biases might be addressed and mitigated with more developed and validated
systems.
  Our research contributes one possible approach to integrating animal ethics
in AI, opening pathways for future studies and practical applications in
various fields, including education, public policy, and regulation, that
involve or relate to animals and society. Overall, this study serves as a step
towards more useful and responsible AI systems that better recognize and
respect the vital interests and perspectives of all sentient beings.

---------------

### 06 Dec 2023 | [Evaluating and Mitigating Discrimination in Language Model Decisions](https://arxiv.org/abs/2312.03689) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.03689)
*Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas  Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, Deep Ganguli* 

  As language models (LMs) advance, interest is growing in applying them to
high-stakes societal decisions, such as determining financing or housing
eligibility. However, their potential for discrimination in such contexts
raises ethical concerns, motivating the need for better methods to evaluate
these risks. We present a method for proactively evaluating the potential
discriminatory impact of LMs in a wide range of use cases, including
hypothetical use cases where they have not yet been deployed. Specifically, we
use an LM to generate a wide array of potential prompts that decision-makers
may input into an LM, spanning 70 diverse decision scenarios across society,
and systematically vary the demographic information in each prompt. Applying
this methodology reveals patterns of both positive and negative discrimination
in the Claude 2.0 model in select settings when no interventions are applied.
While we do not endorse or permit the use of language models to make automated
decisions for the high-risk use cases we study, we demonstrate techniques to
significantly decrease both positive and negative discrimination through
careful prompt engineering, providing pathways toward safer deployment in use
cases where they may be appropriate. Our work enables developers and
policymakers to anticipate, measure, and address discrimination as language
model capabilities and applications continue to expand. We release our dataset
and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval

---------------

### 19 Aug 2022 | [A fast converging particle swarm optimization through targeted,  position-mutated, elitism (PSO-TPME)](https://arxiv.org/abs/2207.00900) | [‚¨áÔ∏è](https://arxiv.org/pdf/2207.00900)
*Tamir Shaqarin and Bernd R. Noack* 

  We dramatically improve convergence speed and global exploration capabilities
of particle swarm optimization (PSO) through a targeted position-mutated
elitism (PSO-TPME). The three key innovations address particle classification,
elitism, and mutation in the cognitive and social model. PSO-TPME is
benchmarked against five popular PSO variants for multi-dimensional functions,
which are extensively adopted in the optimization field, In particular, the
convergence accuracy, convergence speed, and the capability to find global
minima is investigated. The statistical error is assessed by numerous
repetitions. The simulations demonstrate that proposed PSO variant outperforms
the other variants in terms of convergence rate and accuracy by orders of
magnitude.

---------------

### 05 Nov 2023 | [Evaluating the Potential of Leading Large Language Models in Reasoning  Biology Questions](https://arxiv.org/abs/2311.07582) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.07582)
*Xinyu Gong, Jason Holmes, Yiwei Li, Zhengliang Liu, Qi Gan, Zihao Wu,  Jianli Zhang, Yusong Zou, Yuxi Teng, Tian Jiang, Hongtu Zhu, Wei Liu,  Tianming Liu, Yajun Yan* 

  Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.

---------------

### 30 Oct 2023 | [Adversarial Attacks and Defenses in Large Language Models: Old and New  Threats](https://arxiv.org/abs/2310.19737) | [‚¨áÔ∏è](https://arxiv.org/pdf/2310.19737)
*Leo Schwinn and David Dobre and Stephan G\"unnemann and Gauthier Gidel* 

  Over the past decade, there has been extensive research aimed at enhancing
the robustness of neural networks, yet this problem remains vastly unsolved.
Here, one major impediment has been the overestimation of the robustness of new
defense approaches due to faulty defense evaluations. Flawed robustness
evaluations necessitate rectifications in subsequent works, dangerously slowing
down the research and providing a false sense of security. In this context, we
will face substantial challenges associated with an impending adversarial arms
race in natural language processing, specifically with closed-source Large
Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We
provide a first set of prerequisites to improve the robustness assessment of
new approaches and reduce the amount of faulty evaluations. Additionally, we
identify embedding space attacks on LLMs as another viable threat model for the
purposes of generating malicious content in open-sourced models. Finally, we
demonstrate on a recently proposed defense that, without LLM-specific best
practices in place, it is easy to overestimate the robustness of a new
approach.

---------------

### 16 Oct 2023 | [ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI  chatbots at scientific writing?](https://arxiv.org/abs/2309.08636) | [‚¨áÔ∏è](https://arxiv.org/pdf/2309.08636)
*Edisa Lozi\'c and Benjamin \v{S}tular* 

  Historical emphasis on writing mastery has shifted with advances in
generative AI, especially in scientific writing. This study analysed six AI
chatbots for scholarly writing in humanities and archaeology. Using methods
that assessed factual correctness and scientific contribution, ChatGPT-4 showed
the highest quantitative accuracy, closely followed by ChatGPT-3.5, Bing, and
Bard. However, Claude 2 and Aria scored considerably lower. Qualitatively, all
AIs exhibited proficiency in merging existing knowledge, but none produced
original scientific content. Inter-estingly, our findings suggest ChatGPT-4
might represent a plateau in large language model size. This research
emphasizes the unique, intricate nature of human research, suggesting that AI's
emulation of human originality in scientific writing is challenging. As of
2023, while AI has transformed content generation, it struggles with original
contributions in humanities. This may change as AI chatbots continue to evolve
into LLM-powered software.

---------------

### 04 May 2023 | [AutoOpt: A General Framework for Automatically Designing Metaheuristic  Optimization Algorithms with Diverse Structures](https://arxiv.org/abs/2204.00998) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.00998)
*Qi Zhao, Bai Yan, Xianglong Chen, Taiwei Hu, Shi Cheng, Yuhui Shi* 

  Metaheuristics are widely recognized gradient-free solvers to hard problems
that do not meet the rigorous mathematical assumptions of conventional solvers.
The automated design of metaheuristic algorithms provides an attractive path to
relieve manual design effort and gain enhanced performance beyond human-made
algorithms. However, the specific algorithm prototype and linear algorithm
representation in the current automated design pipeline restrict the design
within a fixed algorithm structure, which hinders discovering novelties and
diversity across the metaheuristic family. To address this challenge, this
paper proposes a general framework, AutoOpt, for automatically designing
metaheuristic algorithms with diverse structures. AutoOpt contains three
innovations: (i) A general algorithm prototype dedicated to covering the
metaheuristic family as widely as possible. It promotes high-quality automated
design on different problems by fully discovering potentials and novelties
across the family. (ii) A directed acyclic graph algorithm representation to
fit the proposed prototype. Its flexibility and evolvability enable discovering
various algorithm structures in a single run of design, thus boosting the
possibility of finding high-performance algorithms. (iii) A graph
representation embedding method offering an alternative compact form of the
graph to be manipulated, which ensures AutoOpt's generality. Experiments on
numeral functions and real applications validate AutoOpt's efficiency and
practicability.

---------------

### 06 Mar 2024 | [Benchmarking Hallucination in Large Language Models based on  Unanswerable Math Word Problem](https://arxiv.org/abs/2403.03558) | [‚¨áÔ∏è](https://arxiv.org/pdf/2403.03558)
*Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao* 

  Large language models (LLMs) are highly effective in various natural language
processing (NLP) tasks. However, they are susceptible to producing unreliable
conjectures in ambiguous contexts called hallucination. This paper presents a
new method for evaluating LLM hallucination in Question Answering (QA) based on
the unanswerable math word problem (MWP). To support this approach, we
innovatively develop a dataset called Unanswerable Math Word Problem (UMWP)
which comprises 5200 questions across five categories. We developed an
evaluation methodology combining text similarity and mathematical expression
detection to determine whether LLM considers the question unanswerable. The
results of extensive experiments conducted on 31 LLMs, including GPT-3,
InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and
reinforcement learning with human feedback (RLHF) training significantly
enhance the model's ability to avoid hallucination. We show that utilizing MWP
is a reliable and effective approach to assess hallucination. Our code and data
are available at https://github.com/Yuki-Asuuna/UMWP.

---------------