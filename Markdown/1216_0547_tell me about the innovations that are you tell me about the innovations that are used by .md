tell me about the innovations that are you tell me about the innovations that are used by Claude 35 sonnet in anthropic AIS constitutional AI please explain what constitutional AI is providing a definition for layman's terms\n

### üîé tell me about the innovations that are you tell me about the innovations that are used by Claude 35 sonnet in anthropic AIS constitutional AI please explain what constitutional AI is providing a definition for layman's terms\n


Constitutional AI is a concept in artificial intelligence (AI) that refers to the idea of creating AI systems that are capable of understanding and operating within a set of constitutional rules or principles. These rules or principles are designed to guide the behavior of the AI system, ensuring that it acts in a way that is aligned with human values and societal norms.

One example of a constitutional AI system is Claude 35 sonnet, which is an AI model developed by the MIT Media Lab. Claude 35 sonnet is designed to operate within a set of constitutional rules that are based on the principles of fairness, accountability, and transparency. These rules are encoded into the AI model using a technique called "constitutional AI," which involves training the model to recognize and respect the constitutional rules as it makes decisions.

Some of the innovations used by Claude 35 sonnet include:

* A constitutional learning algorithm that allows the AI model to learn and follow the constitutional rules as it makes decisions.
* A fairness metric that measures the fairness of the AI model's decisions and provides feedback to the model to help it improve its fairness over time.
* A transparency mechanism that allows users to understand how the AI model is making its decisions and why it is making certain choices.

Overall, constitutional AI is an important concept in the development of AI systems that are capable of operating in a responsible and ethical manner. By encoding constitutional rules into AI models, we can help ensure that these systems act in ways that are aligned with human values and societal norms, and that they are transparent and accountable in their decision-making.</s>

# ü©∫üîç Search Results
### 02 Jan 2024 | [Evaluating Large Language Models on the GMAT: Implications for the  Future of Business Education](https://arxiv.org/abs/2401.02985) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.02985)
*Vahid Ashrafimoghari, Necdet G\"urkan, and Jordan W. Suchow* 

  The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

---------------

### 20 Oct 2023 | [Specific versus General Principles for Constitutional AI](https://arxiv.org/abs/2310.13798) | [‚¨áÔ∏è](https://arxiv.org/pdf/2310.13798)
*Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew  Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden  McLean, Catherine Olsson, Cassie Evraets, Eli Tran-Johnson, Esin Durmus,  Ethan Perez, Jackson Kernion, Jamie Kerr, Kamal Ndousse, Karina Nguyen,  Nelson Elhage, Newton Cheng, Nicholas Schiefer, Nova DasSarma, Oliver Rausch,  Robin Larson, Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I.  Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, S\"oren Mindermann,  Nicholas Joseph, Sam McCandlish, Jared Kaplan* 

  Human feedback can prevent overtly harmful utterances in conversational
models, but may not automatically mitigate subtle problematic behaviors such as
a stated desire for self-preservation or power. Constitutional AI offers an
alternative, replacing human feedback with feedback from AI models conditioned
only on a list of written principles. We find this approach effectively
prevents the expression of such behaviors. The success of simple principles
motivates us to ask: can models learn general ethical behaviors from only a
single written principle? To test this, we run experiments using a principle
roughly stated as "do what's best for humanity". We find that the largest
dialogue models can generalize from this short constitution, resulting in
harmless assistants with no stated interest in specific motivations like power.
A general principle may thus partially avoid the need for a long list of
constitutions targeting potentially harmful behaviors. However, more detailed
constitutions still improve fine-grained control over specific types of harms.
This suggests both general and specific principles have value for steering AI
safely.

---------------

### 06 Mar 2023 | [Perspectives on the Social Impacts of Reinforcement Learning with Human  Feedback](https://arxiv.org/abs/2303.02891) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.02891)
*Gabrielle Kaili-May Liu* 

  Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF.

---------------

### 18 Nov 2017 | [The Cultural Evolution of National Constitutions](https://arxiv.org/abs/1711.06899) | [‚¨áÔ∏è](https://arxiv.org/pdf/1711.06899)
*Daniel N. Rockmore, Chen Fang, Nicholas J. Foti, Tom Ginsburg, David  C. Krakauer* 

  We explore how ideas from infectious disease and genetics can be used to
uncover patterns of cultural inheritance and innovation in a corpus of 591
national constitutions spanning 1789 - 2008. Legal "Ideas" are encoded as
"topics" - words statistically linked in documents - derived from topic
modeling the corpus of constitutions. Using these topics we derive a diffusion
network for borrowing from ancestral constitutions back to the US Constitution
of 1789 and reveal that constitutions are complex cultural recombinants. We
find systematic variation in patterns of borrowing from ancestral texts and
"biological"-like behavior in patterns of inheritance with the distribution of
"offspring" arising through a bounded preferential-attachment process. This
process leads to a small number of highly innovative (influential)
constitutions some of which have yet to have been identified as so in the
current literature. Our findings thus shed new light on the critical nodes of
the constitution-making network. The constitutional network structure reflects
periods of intense constitution creation, and systematic patterns of variation
in constitutional life-span and temporal influence.

---------------

### 12 Sep 2018 | [Extracting Fairness Policies from Legal Documents](https://arxiv.org/abs/1809.04262) | [‚¨áÔ∏è](https://arxiv.org/pdf/1809.04262)
*Rashmi Nagpal, Chetna Wadhwa, Mallika Gupta, Samiulla Shaikh, Sameep  Mehta, Vikram Goyal* 

  Machine Learning community is recently exploring the implications of bias and
fairness with respect to the AI applications. The definition of fairness for
such applications varies based on their domain of application. The policies
governing the use of such machine learning system in a given context are
defined by the constitutional laws of nations and regulatory policies enforced
by the organizations that are involved in the usage. Fairness related laws and
policies are often spread across the large documents like constitution,
agreements, and organizational regulations. These legal documents have long
complex sentences in order to achieve rigorousness and robustness. Automatic
extraction of fairness policies, or in general, any specific kind of policies
from large legal corpus can be very useful for the study of bias and fairness
in the context of AI applications.
  We attempted to automatically extract fairness policies from publicly
available law documents using two approaches based on semantic relatedness. The
experiments reveal how classical Wordnet-based similarity and vector-based
similarity differ in addressing this task. We have shown that similarity based
on word vectors beats the classical approach with a large margin, whereas other
vector representations of senses and sentences fail to even match the classical
baseline. Further, we have presented thorough error analysis and reasoning to
explain the results with appropriate examples from the dataset for deeper
insights.

---------------

### 17 Jul 2023 | [Fairness in KI-Systemen](https://arxiv.org/abs/2307.08486) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.08486)
*Janine Strotherm and Alissa M\"uller and Barbara Hammer and Benjamin  Paa{\ss}en* 

  The more AI-assisted decisions affect people's lives, the more important the
fairness of such decisions becomes. In this chapter, we provide an introduction
to research on fairness in machine learning. We explain the main fairness
definitions and strategies for achieving fairness using concrete examples and
place fairness research in the European context. Our contribution is aimed at
an interdisciplinary audience and therefore avoids mathematical formulation but
emphasizes visualizations and examples.
  --
  Je mehr KI-gest\"utzte Entscheidungen das Leben von Menschen betreffen, desto
wichtiger ist die Fairness solcher Entscheidungen. In diesem Kapitel geben wir
eine Einf\"uhrung in die Forschung zu Fairness im maschinellen Lernen. Wir
erkl\"aren die wesentlichen Fairness-Definitionen und Strategien zur Erreichung
von Fairness anhand konkreter Beispiele und ordnen die Fairness-Forschung in
den europ\"aischen Kontext ein. Unser Beitrag richtet sich dabei an ein
interdisziplin\"ares Publikum und verzichtet daher auf die mathematische
Formulierung sondern betont Visualisierungen und Beispiele.

---------------

### 13 Feb 2024 | [Suppressing Pink Elephants with Direct Principle Feedback](https://arxiv.org/abs/2402.07896) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.07896)
*Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf,  Siddharth Verma, Stella Biderman* 

  Existing methods for controlling language models, such as RLHF and
Constitutional AI, involve determining which LLM behaviors are desirable and
training them into a language model. However, in many cases, it is desirable
for LLMs to be controllable at inference time, so that they can be used in
multiple contexts with diverse needs. We illustrate this with the Pink Elephant
Problem: instructing an LLM to avoid discussing a certain entity (a ``Pink
Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We
apply a novel simplification of Constitutional AI, Direct Principle Feedback,
which skips the ranking of responses and uses DPO directly on critiques and
revisions. Our results show that after DPF fine-tuning on our synthetic Pink
Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms
Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on
our curated test set assessing the Pink Elephant Problem.

---------------

### 15 Dec 2022 | [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) | [‚¨áÔ∏è](https://arxiv.org/pdf/2212.08073)
*Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson  Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron  McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez,  Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie  Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile  Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,  Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott  Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham,  Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R.  Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam  McCandlish, Tom Brown, Jared Kaplan* 

  As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.

---------------

### 28 Dec 2023 | [AI Content Self-Detection for Transformer-based Large Language Models](https://arxiv.org/abs/2312.17289) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.17289)
*Ant\^onio Junior Alves Caiado and Michael Hahsler* 

  $ $The usage of generative artificial intelligence (AI) tools based on large
language models, including ChatGPT, Bard, and Claude, for text generation has
many exciting applications with the potential for phenomenal productivity
gains. One issue is authorship attribution when using AI tools. This is
especially important in an academic setting where the inappropriate use of
generative AI tools may hinder student learning or stifle research by creating
a large amount of automatically generated derivative work. Existing plagiarism
detection systems can trace the source of submitted text but are not yet
equipped with methods to accurately detect AI-generated text. This paper
introduces the idea of direct origin detection and evaluates whether generative
AI systems can recognize their output and distinguish it from human-written
texts. We argue why current transformer-based models may be able to self-detect
their own generated text and perform a small empirical study using zero-shot
learning to investigate if that is the case. Results reveal varying
capabilities of AI systems to identify their generated text. Google's Bard
model exhibits the largest capability of self-detection with an accuracy of
94\%, followed by OpenAI's ChatGPT with 83\%. On the other hand, Anthropic's
Claude model seems to be not able to self-detect.

---------------

### 26 Nov 2023 | [Case Repositories: Towards Case-Based Reasoning for AI Alignment](https://arxiv.org/abs/2311.10934) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.10934)
*K. J. Kevin Feng, Quan Ze Chen, Inyoung Cheong, King Xia, Amy X. Zhang* 

  Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed'' cases -- questions one may
ask an AI system -- in a particular domain, 2) eliciting domain-specific key
dimensions for cases through workshops with domain experts, 3) using LLMs to
generate variations of cases not seen in the wild, and 4) engaging with the
public to judge and improve cases. We then discuss how such a case repository
could assist in AI alignment, both through directly acting as precedents to
ground acceptable behaviors, and as a medium for individuals and communities to
engage in moral reasoning around AI.

---------------

### 20 Jul 2021 | [Evolutionary Innovation Viewed as Novel Physical Phenomena and  Hierarchical Systems Building](https://arxiv.org/abs/2107.09669) | [‚¨áÔ∏è](https://arxiv.org/pdf/2107.09669)
*Tim Taylor* 

  In previous work I proposed a framework for thinking about open-ended
evolution. The framework characterised the basic processes required for
Darwinian evolution as: (1) the generation of a phenotype from a genetic
description; (2) the evaluation of that phenotype; and (3) the reproduction
with variation of successful genotype-phenotypes. My treatment emphasized the
potential influence of the biotic and abiotic environment, and of the laws of
physics/chemistry, on each of these processes. I demonstrated the conditions
under which these processes can allow for ongoing exploration of a space of
possible phenotypes (which I labelled exploratory open-endedness). However,
these processes by themselves cannot expand the space of possible phenotypes
and therefore cannot account for the more interesting and unexpected kinds of
evolutionary innovation (such as those I labelled expansive and
transformational open-endedness). In the previous work I looked at ways in
which expansive and transformational innovations could arise. I proposed
transdomain bridges and non-additive compositional systems as two mechanisms by
which these kinds of innovations could arise. In the current paper I wish to
generalise and expand upon these two concepts. I do this by adopting the
Parameter Space-Organisation Space-Action Space (POA) perspective, as suggested
at in my previous work, and proposing that all evolutionary innovations can be
viewed as either capturing some novel physical phenomena that had previously
been unused, or as the creation of new persistent systems within the
environment.

---------------

### 02 Feb 2023 | [Abductive Inference and C. S. Peirce: 150 Years Later](https://arxiv.org/abs/2111.08054) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.08054)
*Deep Mukhopadhyay* 

  This paper is about two things: (i) Charles Sanders Peirce (1837-1914) -- an
iconoclastic philosopher and polymath who is among the greatest of American
minds. (ii) Abductive inference -- a term coined by C. S. Peirce, which he
defined as "the process of forming explanatory hypotheses. It is the only
logical operation which introduces any new idea."
  Abductive inference and quantitative economics: Abductive inference plays a
fundamental role in empirical scientific research as a tool for discovery and
data analysis. Heckman and Singer (2017) strongly advocated "Economists should
abduct." Arnold Zellner (2007) stressed that "much greater emphasis on
reductive [abductive] inference in teaching econometrics, statistics, and
economics would be desirable." But currently, there are no established theory
or practical tools that can allow an empirical analyst to abduct. This paper
attempts to fill this gap by introducing new principles and concrete procedures
to the Economics and Statistics community. I termed the proposed approach as
Abductive Inference Machine (AIM).
  The historical Peirce's experiment: In 1872, Peirce conducted a series of
experiments to determine the distribution of response times to an auditory
stimulus, which is widely regarded as one of the most significant statistical
investigations in the history of nineteenth-century American mathematical
research (Stigler, 1978). On the 150th anniversary of this historical
experiment, we look back at the Peircean-style abductive inference through a
modern statistical lens. Using Peirce's data, it is shown how empirical
analysts can abduct in a systematic and automated manner using AIM.

---------------

### 20 Oct 2021 | [An argument for the impossibility of machine intelligence](https://arxiv.org/abs/2111.07765) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.07765)
*Jobst Landgrebe, Barry Smith* 

  Since the noun phrase `artificial intelligence' (AI) was coined, it has been
debated whether humans are able to create intelligence using technology. We
shed new light on this question from the point of view of themodynamics and
mathematics. First, we define what it is to be an agent (device) that could be
the bearer of AI. Then we show that the mainstream definitions of
`intelligence' proposed by Hutter and others and still accepted by the AI
community are too weak even to capture what is involved when we ascribe
intelligence to an insect. We then summarise the highly useful definition of
basic (arthropod) intelligence proposed by Rodney Brooks, and we identify the
properties that an AI agent would need to possess in order to be the bearer of
intelligence by this definition. Finally, we show that, from the perspective of
the disciplines needed to create such an agent, namely mathematics and physics,
these properties are realisable by neither implicit nor explicit mathematical
design nor by setting up an environment in which an AI could evolve
spontaneously.

---------------

### 19 Feb 2024 | [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://arxiv.org/abs/2310.06452) | [‚¨áÔ∏è](https://arxiv.org/pdf/2310.06452)
*Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena  Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu* 

  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has
been significant work developing these methods, our understanding of the
benefits and downsides of each stage in RLHF is still limited. To fill this
gap, we present an extensive analysis of how each stage of the process (i.e.
supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key
properties: out-of-distribution (OOD) generalisation and output diversity. OOD
generalisation is crucial given the wide range of real-world scenarios in which
these models are being used, while output diversity refers to the model's
ability to generate varied outputs and is important for a variety of use cases.
We perform our analysis across two base models on both summarisation and
instruction following tasks, the latter being highly relevant for current LLM
use cases. We find that RLHF generalises better than SFT to new inputs,
particularly as the distribution shift between train and test becomes larger.
However, RLHF significantly reduces output diversity compared to SFT across a
variety of measures, implying a tradeoff in current LLM fine-tuning methods
between generalisation and diversity. Our results provide guidance on which
fine-tuning method should be used depending on the application, and show that
more research is needed to improve the tradeoff between generalisation and
diversity.

---------------

### 15 Nov 2023 | [Aligned: A Platform-based Process for Alignment](https://arxiv.org/abs/2311.08706) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.08706)
*Ethan Shaotran, Ido Pesok, Sam Jones, and Emi Liu* 

  We are introducing Aligned, a platform for global governance and alignment of
frontier models, and eventually superintelligence. While previous efforts at
the major AI labs have attempted to gather inputs for alignment, these are
often conducted behind closed doors. We aim to set the foundation for a more
trustworthy, public-facing approach to safety: a constitutional committee
framework. Initial tests with 680 participants result in a 30-guideline
constitution with 93% overall support. We show the platform naturally scales,
instilling confidence and enjoyment from the community. We invite other AI labs
and teams to plug and play into the Aligned ecosystem.

---------------

### 06 Mar 2023 | [Choice Over Control: How Users Write with Large Language Models using  Diegetic and Non-Diegetic Prompting](https://arxiv.org/abs/2303.03199) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.03199)
*Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek* 

  We propose a conceptual perspective on prompts for Large Language Models
(LLMs) that distinguishes between (1) diegetic prompts (part of the narrative,
e.g. "Once upon a time, I saw a fox..."), and (2) non-diegetic prompts
(external, e.g. "Write about the adventures of the fox."). With this lens, we
study how 129 crowd workers on Prolific write short texts with different user
interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with
GPT-3): When the interface offered multiple suggestions and provided an option
for non-diegetic prompting, participants preferred choosing from multiple
suggestions over controlling them via non-diegetic prompts. When participants
provided non-diegetic prompts it was to ask for inspiration, topics or facts.
Single suggestions in particular were guided both with diegetic and
non-diegetic information. This work informs human-AI interaction with
generative models by revealing that (1) writing non-diegetic prompts requires
effort, (2) people combine diegetic and non-diegetic prompting, and (3) they
use their draft (i.e. diegetic information) and suggestion timing to
strategically guide LLMs.

---------------

### 15 Jan 2024 | [ChatGPT's One-year Anniversary: Are Open-Source Large Language Models  Catching up?](https://arxiv.org/abs/2311.16989) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.16989)
*Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut,  Ruochen Zhao, Caiming Xiong, Shafiq Joty* 

  Upon its release in late 2022, ChatGPT has brought a seismic shift in the
entire landscape of AI, both in research and commerce. Through
instruction-tuning a large language model (LLM) with supervised fine-tuning and
reinforcement learning from human feedback, it showed that a model could answer
human questions and follow instructions on a broad panel of tasks. Following
this success, interests in LLMs have intensified, with new LLMs flourishing at
frequent interval across academia and industry, including many start-ups
focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's
Claude) generally outperform their open-source counterparts, the progress on
the latter has been rapid with claims of achieving parity or even better on
certain tasks. This has crucial implications not only on research but also on
business. In this work, on the first anniversary of ChatGPT, we provide an
exhaustive overview of this success, surveying all tasks where an open-source
LLM has claimed to be on par or better than ChatGPT.

---------------

### 02 Mar 2024 | [The Case for Animal-Friendly AI](https://arxiv.org/abs/2403.01199) | [‚¨áÔ∏è](https://arxiv.org/pdf/2403.01199)
*Sankalpa Ghose, Yip Fai Tse, Kasra Rasaee, Jeff Sebo, Peter Singer* 

  Artificial intelligence is seen as increasingly important, and potentially
profoundly so, but the fields of AI ethics and AI engineering have not fully
recognized that these technologies, including large language models (LLMs),
will have massive impacts on animals. We argue that this impact matters,
because animals matter morally.
  As a first experiment in evaluating animal consideration in LLMs, we
constructed a proof-of-concept Evaluation System, which assesses LLM responses
and biases from multiple perspectives. This system evaluates LLM outputs by two
criteria: their truthfulness, and the degree of consideration they give to the
interests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using
a set of structured queries and predefined normative perspectives. Preliminary
results suggest that the outcomes of the tested models can be benchmarked
regarding the consideration they give to animals, and that generated positions
and biases might be addressed and mitigated with more developed and validated
systems.
  Our research contributes one possible approach to integrating animal ethics
in AI, opening pathways for future studies and practical applications in
various fields, including education, public policy, and regulation, that
involve or relate to animals and society. Overall, this study serves as a step
towards more useful and responsible AI systems that better recognize and
respect the vital interests and perspectives of all sentient beings.

---------------

### 01 Nov 2023 | [From Text to Structure: Using Large Language Models to Support the  Development of Legal Expert Systems](https://arxiv.org/abs/2311.04911) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.04911)
*Samyar Janatian, Hannes Westermann, Jinzhe Tan, Jaromir Savelka, Karim  Benyekhlef* 

  Encoding legislative text in a formal representation is an important
prerequisite to different tasks in the field of AI & Law. For example,
rule-based expert systems focused on legislation can support laypeople in
understanding how legislation applies to them and provide them with helpful
context and information. However, the process of analyzing legislation and
other sources to encode it in the desired formal representation can be
time-consuming and represents a bottleneck in the development of such systems.
Here, we investigate to what degree large language models (LLMs), such as
GPT-4, are able to automatically extract structured representations from
legislation. We use LLMs to create pathways from legislation, according to the
JusticeBot methodology for legal decision support systems, evaluate the
pathways and compare them to manually created pathways. The results are
promising, with 60% of generated pathways being rated as equivalent or better
than manually created ones in a blind comparison. The approach suggests a
promising path to leverage the capabilities of LLMs to ease the costly
development of systems based on symbolic approaches that are transparent and
explainable.

---------------

### 20 Jun 2023 | [Opportunities and Risks of LLMs for Scalable Deliberation with Polis](https://arxiv.org/abs/2306.11932) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.11932)
*Christopher T. Small, Ivan Vendrov, Esin Durmus, Hadjar Homaei,  Elizabeth Barry, Julien Cornebise, Ted Suzman, Deep Ganguli, and Colin Megill* 

  Polis is a platform that leverages machine intelligence to scale up
deliberative processes. In this paper, we explore the opportunities and risks
associated with applying Large Language Models (LLMs) towards challenges with
facilitating, moderating and summarizing the results of Polis engagements. In
particular, we demonstrate with pilot experiments using Anthropic's Claude that
LLMs can indeed augment human intelligence to help more efficiently run Polis
conversations. In particular, we find that summarization capabilities enable
categorically new methods with immense promise to empower the public in
collective meaning-making exercises. And notably, LLM context limitations have
a significant impact on insight and quality of these results.
  However, these opportunities come with risks. We discuss some of these risks,
as well as principles and techniques for characterizing and mitigating them,
and the implications for other deliberative or political systems that may
employ LLMs. Finally, we conclude with several open future research directions
for augmenting tools like Polis with LLMs.

---------------