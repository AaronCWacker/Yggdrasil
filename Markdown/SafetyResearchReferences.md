summarize the top 10 Ai and triage medical or healthcare related evaluations including human in the loop practices for being able to evaluate performance metrics of AI along with\n studies of triage that are published acceptance test cases\n for triage accuracy\n and understanding of complex differential diagnosis\n

# ü©∫üîç Search Results
### 27 Jun 2018 | [A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis](https://arxiv.org/abs/1806.10698) | [‚¨áÔ∏è](https://arxiv.org/pdf/1806.10698)
*Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri* 

  Online symptom checkers have significant potential to improve patient care,
however their reliability and accuracy remain variable. We hypothesised that an
artificial intelligence (AI) powered triage and diagnostic system would compare
favourably with human doctors with respect to triage and diagnostic accuracy.
We performed a prospective validation study of the accuracy and safety of an AI
powered triage and diagnostic system. Identical cases were evaluated by both an
AI system and human doctors. Differential diagnoses and triage outcomes were
evaluated by an independent judge, who was blinded from knowing the source (AI
system or human doctor) of the outcomes. Independently of these cases,
vignettes from publicly available resources were also assessed to provide a
benchmark to previous studies and the diagnostic component of the MRCGP exam.
Overall we found that the Babylon AI powered Triage and Diagnostic System was
able to identify the condition modelled by a clinical vignette with accuracy
comparable to human doctors (in terms of precision and recall). In addition, we
found that the triage advice recommended by the AI System was, on average,
safer than that of human doctors, when compared to the ranges of acceptable
triage provided by independent expert judges, with only a minimal reduction in
appropriateness.

---------------

### 09 May 2023 | [The Case Records of ChatGPT: Language Models and Complex Clinical  Questions](https://arxiv.org/abs/2305.05609) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.05609)
*Timothy Poterucha, Pierre Elias, Christopher M. Haggerty* 

  Background: Artificial intelligence language models have shown promise in
various applications, including assisting with clinical decision-making as
demonstrated by strong performance of large language models on medical
licensure exams. However, their ability to solve complex, open-ended cases,
which may be representative of clinical practice, remains unexplored. Methods:
In this study, the accuracy of large language AI models GPT4 and GPT3.5 in
diagnosing complex clinical cases was investigated using published Case Records
of the Massachusetts General Hospital. A total of 50 cases requiring a
diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022
were identified. For each case, models were given a prompt requesting the top
three specific diagnoses and associated diagnostic tests, followed by case
text, labs, and figure legends. Model outputs were assessed in comparison to
the final clinical diagnosis and whether the model-predicted test would result
in a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the
correct diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%
within three attempts, respectively. GPT4 and GPT3.5 provided a correct
essential diagnostic test in 28% and 24% of cases in one attempt, and 44% and
50% within three attempts, respectively. No significant differences were found
between the two models, and multiple trials with identical prompts using the
GPT3.5 model provided similar results. Conclusions: In summary, these models
demonstrate potential usefulness in generating differential diagnoses but
remain limited in their ability to provide a single unifying diagnosis in
complex, open-ended cases. Future research should focus on evaluating model
performance in larger datasets of open-ended clinical challenges and exploring
potential human-AI collaboration strategies to enhance clinical
decision-making.

---------------

### 28 May 2021 | [Can artificial intelligence (AI) be used to accurately detect  tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB  screening and triaging in a high TB burden setting](https://arxiv.org/abs/2006.05509) | [‚¨áÔ∏è](https://arxiv.org/pdf/2006.05509)
*Zhi Zhen Qin, Shahriar Ahmed, Mohammad Shahnewaz Sarker, Kishor Paul,  Ahammad Shafiq Sikder Adel, Tasneem Naheyan, Rachael Barrett, Sayera Banu,  Jacob Creswell* 

  Artificial intelligence (AI) products can be trained to recognize
tuberculosis (TB)-related abnormalities on chest radiographs. Various AI
products are available commercially, yet there is lack of evidence on how their
performance compared with each other and with radiologists. We evaluated five
AI software products for screening and triaging TB using a large dataset that
had not been used to train any commercial AI products. Individuals (>=15 years
old) presenting to three TB screening centers in Dhaka, Bangladesh, were
recruited consecutively. All CXR were read independently by a group of three
Bangladeshi registered radiologists and five commercial AI products: CAD4TB
(v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR
(v3). All five AI products significantly outperformed the Bangladeshi
radiologists. The areas under the receiver operating characteristic curve are
qXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit
INSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI:
84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP
with 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the
number of Xpert tests required by 50%, while maintaining a sensitivity above
90%. All AI algorithms performed worse among the older age and people with
prior TB history. AI products can be highly accurate and useful screening and
triage tools for TB detection in high burden regions and outperform human
readers.

---------------

### 11 May 2022 | [Performance of a deep learning system for detection of referable  diabetic retinopathy in real clinical settings](https://arxiv.org/abs/2205.05554) | [‚¨áÔ∏è](https://arxiv.org/pdf/2205.05554)
*Ver\'onica S\'anchez-Guti\'errez, Paula Hern\'andez-Mart\'inez,  Francisco J. Mu\~noz-Negrete, Jonne Engelberts, Allison M. Luger, Mark J.J.P.  van Grinsven* 

  Background: To determine the ability of a commercially available deep
learning system, RetCAD v.1.3.1 (Thirona, Nijmegen, The Netherlands) for the
automatic detection of referable diabetic retinopathy (DR) on a dataset of
colour fundus images acquired during routine clinical practice in a tertiary
hospital screening program, analyzing the reduction of workload that can be
released incorporating this artificial intelligence-based technology. Methods:
Evaluation of the software was performed on a dataset of 7195 nonmydriatic
fundus images from 6325 eyes of 3189 diabetic patients attending our screening
program between February to December of 2019. The software generated a DR
severity score for each colour fundus image which was combined into an
eye-level score. This score was then compared with a reference standard as set
by a human expert using receiver operating characteristic (ROC) curve analysis.
Results: The artificial intelligence (AI) software achieved an area under the
ROC curve (AUC) value of 0.988 [0.981:0.993] for the detection of referable DR.
At the proposed operating point, the sensitivity of the RetCAD software for DR
is 90.53% and specificity is 97.13%. A workload reduction of 96% could be
achieved at the cost of only 6 false negatives. Conclusions: The AI software
correctly identified the vast majority of referable DR cases, with a workload
reduction of 96% of the cases that would need to be checked, while missing
almost no true cases, so it may therefore be used as an instrument for triage.

---------------

### 18 Feb 2022 | [An Integrated Optimization and Machine Learning Models to Predict the  Admission Status of Emergency Patients](https://arxiv.org/abs/2202.09196) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.09196)
*Abdulaziz Ahmed, Omar Ashour, Haneen Ali, Mohammad Firouz* 

  This work proposes a framework for optimizing machine learning algorithms.
The practicality of the framework is illustrated using an important case study
from the healthcare domain, which is predicting the admission status of
emergency department (ED) patients (e.g., admitted vs. discharged) using
patient data at the time of triage. The proposed framework can mitigate the
crowding problem by proactively planning the patient boarding process. A large
retrospective dataset of patient records is obtained from the electronic health
record database of all ED visits over three years from three major locations of
a healthcare provider in the Midwest of the US. Three machine learning
algorithms are proposed: T-XGB, T-ADAB, and T-MLP. T-XGB integrates extreme
gradient boosting (XGB) and Tabu Search (TS), T-ADAB integrates Adaboost and
TS, and T-MLP integrates multi-layer perceptron (MLP) and TS. The proposed
algorithms are compared with the traditional algorithms: XGB, ADAB, and MLP, in
which their parameters are tunned using grid search. The three proposed
algorithms and the original ones are trained and tested using nine data groups
that are obtained from different feature selection methods. In other words, 54
models are developed. Performance was evaluated using five measures: Area under
the curve (AUC), sensitivity, specificity, F1, and accuracy. The results show
that the newly proposed algorithms resulted in high AUC and outperformed the
traditional algorithms. The T-ADAB performs the best among the newly developed
algorithms. The AUC, sensitivity, specificity, F1, and accuracy of the best
model are 95.4%, 99.3%, 91.4%, 95.2%, 97.2%, respectively.

---------------

### 07 Jan 2022 | [Monitoring Covid-19 on social media using a novel triage and diagnosis  approach](https://arxiv.org/abs/2103.11850) | [‚¨áÔ∏è](https://arxiv.org/pdf/2103.11850)
*Abul Hasan, Mark Levene, David Weston, Renate Fromson, Nicolas  Koslover, and Tamara Levene* 

  Objective: This study aims to develop an end-to-end natural language
processing pipeline for triage and diagnosis of COVID-19 from patient-authored
social media posts, in order to provide researchers and public health
practitioners with additional information on the symptoms, severity and
prevalence of the disease rather than to provide an actionable decision at the
individual level. Materials and Methods: The text processing pipeline first
extracts COVID-19 symptoms and related concepts such as severity, duration,
negations, and body parts from patients' posts using conditional random fields.
An unsupervised rule-based algorithm is then applied to establish relations
between concepts in the next step of the pipeline. The extracted concepts and
relations are subsequently used to construct two different vector
representations of each post. These vectors are applied separately to build
support vector machine learning models to triage patients into three categories
and diagnose them for COVID-19. Results: We report that macro- and
micro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for
the triage and diagnosis of COVID-19, when the models are trained on human
labelled data. Our experimental results indicate that similar performance can
be achieved when the models are trained using predicted labels from concept
extraction and rule-based classifiers, thus yielding end-to-end machine
learning. Also, we highlight important features uncovered by our diagnostic
machine learning models and compare them with the most frequent symptoms
revealed in another COVID-19 dataset. In particular, we found that the most
important features are not always the most frequent ones.

---------------

### 29 Jun 2023 | [Improving Patient Pre-screening for Clinical Trials: Assisting  Physicians with Large Language Models](https://arxiv.org/abs/2304.07396) | [‚¨áÔ∏è](https://arxiv.org/pdf/2304.07396)
*Danny M. den Hamer, Perry Schoor, Tobias B. Polak and Daniel Kapitan* 

  Physicians considering clinical trials for their patients are met with the
laborious process of checking many text based eligibility criteria. Large
Language Models (LLMs) have shown to perform well for clinical information
extraction and clinical reasoning, including medical tests, but not yet in
real-world scenarios. This paper investigates the use of InstructGPT to assist
physicians in determining eligibility for clinical trials based on a patient's
summarised medical profile. Using a prompting strategy combining one-shot,
selection-inference and chain-of-thought techniques, we investigate the
performance of LLMs on 10 synthetically created patient profiles. Performance
is evaluated at four levels: ability to identify screenable eligibility
criteria from a trial given a medical profile; ability to classify for each
individual criterion whether the patient qualifies; the overall classification
whether a patient is eligible for a clinical trial and the percentage of
criteria to be screened by physician. We evaluated against 146 clinical trials
and a total of 4,135 eligibility criteria. The LLM was able to correctly
identify the screenability of 72% (2,994/4,135) of the criteria. Additionally,
72% (341/471) of the screenable criteria were evaluated correctly. The
resulting trial level classification as eligible or ineligible resulted in a
recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0
and precision of 0.71 on clinical trial level can be achieved while reducing
the amount of criteria to be checked by an estimated 90%. LLMs can be used to
assist physicians with pre-screening of patients for clinical trials. By
forcing instruction-tuned LLMs to produce chain-of-thought responses, the
reasoning can be made transparent to and the decision process becomes amenable
by physicians, thereby making such a system feasible for use in real-world
scenarios.

---------------

### 14 Jan 2024 | [Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by  Integrating Expert Knowledge and Interpretable Data-driven Intelligence](https://arxiv.org/abs/2401.08695) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.08695)
*Zhengqing Fang, Shuowen Zhou, Zhouhang Yuan, Yuxuan Si, Mengze Li,  Jinxu Li, Yesheng Xu, Wenjia Xie, Kun Kuang, Yingming Li, Fei Wu, and Yu-Feng  Yao* 

  Although data-driven artificial intelligence (AI) in medical image diagnosis
has shown impressive performance in silico, the lack of interpretability makes
it difficult to incorporate the "black box" into clinicians' workflows. To make
the diagnostic patterns learned from data understandable by clinicians, we
develop an interpretable model, knowledge-guided diagnosis model (KGDM), that
provides a visualized reasoning process containing AI-based biomarkers and
retrieved cases that with the same diagnostic patterns. It embraces clinicians'
prompts into the interpreted reasoning through human-AI interaction, leading to
potentially enhanced safety and more accurate predictions. This study
investigates the performance, interpretability, and clinical utility of KGDM in
the diagnosis of infectious keratitis (IK), which is the leading cause of
corneal blindness. The classification performance of KGDM is evaluated on a
prospective validation dataset, an external testing dataset, and an publicly
available testing dataset. The diagnostic odds ratios (DOR) of the interpreted
AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit
consistent diagnostic patterns with clinic experience. Moreover, a human-AI
collaborative diagnosis test is conducted and the participants with
collaboration achieved a performance exceeding that of both humans and AI. By
synergistically integrating interpretability and interaction, this study
facilitates the convergence of clinicians' expertise and data-driven
intelligence. The promotion of inexperienced ophthalmologists with the aid of
AI-based biomarkers, as well as increased AI prediction by intervention from
experienced ones, demonstrate a promising diagnostic paradigm for infectious
keratitis using KGDM, which holds the potential for extension to other diseases
where experienced medical practitioners are limited and the safety of AI is
concerned.

---------------

### 28 Jun 2021 | [Using machine learning techniques to predict hospital admission at the  emergency department](https://arxiv.org/abs/2106.12921) | [‚¨áÔ∏è](https://arxiv.org/pdf/2106.12921)
*Georgios Feretzakis, George Karlis, Evangelos Loupelis, Dimitris  Kalles, Rea Chatzikyriakou, Nikolaos Trakas, Eugenia Karakou, Aikaterini  Sakagianni, Lazaros Tzelves, Stavroula Petropoulou, Aikaterini Tika, Ilias  Dalainas and Vasileios Kaldis* 

  Introduction: One of the most important tasks in the Emergency Department
(ED) is to promptly identify the patients who will benefit from hospital
admission. Machine Learning (ML) techniques show promise as diagnostic aids in
healthcare. Material and methods: We investigated the following features
seeking to investigate their performance in predicting hospital admission:
serum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,
C-Reactive Protein, Complete Blood Count with differential, Activated Partial
Thromboplastin Time, D Dimer, International Normalized Ratio, age, gender,
triage disposition to ED unit and ambulance utilization. A total of 3,204 ED
visits were analyzed. Results: The proposed algorithms generated models which
demonstrated acceptable performance in predicting hospital admission of ED
patients. The range of F-measure and ROC Area values of all eight evaluated
algorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The
main advantages of this tool include easy access, availability, yes/no result,
and low cost. The clinical implications of our approach might facilitate a
shift from traditional clinical decision-making to a more sophisticated model.
Conclusion: Developing robust prognostic models with the utilization of common
biomarkers is a project that might shape the future of emergency medicine. Our
findings warrant confirmation with implementation in pragmatic ED trials.

---------------

### 13 Sep 2018 | [Focus Group on Artificial Intelligence for Health](https://arxiv.org/abs/1809.04797) | [‚¨áÔ∏è](https://arxiv.org/pdf/1809.04797)
*Marcel Salath\'e, Thomas Wiegand, Markus Wenzel* 

  Artificial Intelligence (AI) - the phenomenon of machines being able to solve
problems that require human intelligence - has in the past decade seen an
enormous rise of interest due to significant advances in effectiveness and use.
The health sector, one of the most important sectors for societies and
economies worldwide, is particularly interesting for AI applications, given the
ongoing digitalisation of all types of health information. The potential for AI
assistance in the health domain is immense, because AI can support medical
decision making at reduced costs, everywhere. However, due to the complexity of
AI algorithms, it is difficult to distinguish good from bad AI-based solutions
and to understand their strengths and weaknesses, which is crucial for
clarifying responsibilities and for building trust. For this reason, the
International Telecommunication Union (ITU) has established a new Focus Group
on "Artificial Intelligence for Health" (FG-AI4H) in partnership with the World
Health Organization (WHO). Health and care services are usually the
responsibility of a government - even when provided through private insurance
systems - and thus under the responsibility of WHO/ITU member states. FG-AI4H
will identify opportunities for international standardization, which will
foster the application of AI to health issues on a global scale. In particular,
it will establish a standardized assessment framework with open benchmarks for
the evaluation of AI-based methods for health, such as AI-based diagnosis,
triage or treatment decisions.

---------------

### 31 Aug 2022 | [Robustness of an Artificial Intelligence Solution for Diagnosis of  Normal Chest X-Rays](https://arxiv.org/abs/2209.09204) | [‚¨áÔ∏è](https://arxiv.org/pdf/2209.09204)
*Tom Dyer, Jordan Smith, Gaetan Dissez, Nicole Tay, Qaiser Malik, Tom  Naunton Morgan, Paul Williams, Liliana Garcia-Mondragon, George Pearse, and  Simon Rasalingham* 

  Purpose: Artificial intelligence (AI) solutions for medical diagnosis require
thorough evaluation to demonstrate that performance is maintained for all
patient sub-groups and to ensure that proposed improvements in care will be
delivered equitably. This study evaluates the robustness of an AI solution for
the diagnosis of normal chest X-rays (CXRs) by comparing performance across
multiple patient and environmental subgroups, as well as comparing AI errors
with those made by human experts.
  Methods: A total of 4,060 CXRs were sampled to represent a diverse dataset of
NHS patients and care settings. Ground-truth labels were assigned by a
3-radiologist panel. AI performance was evaluated against assigned labels and
sub-groups analysis was conducted against patient age and sex, as well as CXR
view, modality, device manufacturer and hospital site.
  Results: The AI solution was able to remove 18.5% of the dataset by
classification as High Confidence Normal (HCN). This was associated with a
negative predictive value (NPV) of 96.0%, compared to 89.1% for diagnosis of
normal scans by radiologists. In all AI false negative (FN) cases, a
radiologist was found to have also made the same error when compared to final
ground-truth labels. Subgroup analysis showed no statistically significant
variations in AI performance, whilst reduced normal classification was observed
in data from some hospital sites.
  Conclusion: We show the AI solution could provide meaningful workload savings
by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers. The
AI solution is shown to perform well across patient subgroups and error cases
were shown to be subjective or subtle in nature.

---------------

### 11 Dec 2023 | [The Limits of Fair Medical Imaging AI In The Wild](https://arxiv.org/abs/2312.10083) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.10083)
*Yuzhe Yang, Haoran Zhang, Judy W Gichoya, Dina Katabi, Marzyeh  Ghassemi* 

  As artificial intelligence (AI) rapidly approaches human-level performance in
medical imaging, it is crucial that it does not exacerbate or propagate
healthcare disparities. Prior research has established AI's capacity to infer
demographic data from chest X-rays, leading to a key concern: do models using
demographic shortcuts have unfair predictions across subpopulations? In this
study, we conduct a thorough investigation into the extent to which medical AI
utilizes demographic encodings, focusing on potential fairness discrepancies
within both in-distribution training sets and external test sets. Our analysis
covers three key medical imaging disciplines: radiology, dermatology, and
ophthalmology, and incorporates data from six global chest X-ray datasets. We
confirm that medical imaging AI leverages demographic shortcuts in disease
classification. While correcting shortcuts algorithmically effectively
addresses fairness gaps to create "locally optimal" models within the original
data distribution, this optimality is not true in new test settings.
Surprisingly, we find that models with less encoding of demographic attributes
are often most "globally optimal", exhibiting better fairness during model
evaluation in new test environments. Our work establishes best practices for
medical imaging models which maintain their performance and fairness in
deployments beyond their initial training contexts, underscoring critical
considerations for AI clinical deployments across populations and sites.

---------------

### 04 Sep 2020 | [A New Screening Method for COVID-19 based on Ocular Feature Recognition  by Machine Learning Tools](https://arxiv.org/abs/2009.03184) | [‚¨áÔ∏è](https://arxiv.org/pdf/2009.03184)
*Yanwei Fu, Feng Li, Wenxuan Wang, Haicheng Tang, Xuelin Qian, Mengwei  Gu, Xiangyang Xue* 

  The Coronavirus disease 2019 (COVID-19) has affected several million people.
With the outbreak of the epidemic, many researchers are devoting themselves to
the COVID-19 screening system. The standard practices for rapid risk screening
of COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction).
However, these methods demand professional efforts of the acquisition of CT
images and saliva samples, a certain amount of waiting time, and most
importantly prohibitive examination fee in some countries. Recently, some
literatures have shown that the COVID-19 patients usually accompanied by ocular
manifestations consistent with the conjunctivitis, including conjunctival
hyperemia, chemosis, epiphora, or increased secretions. After more than four
months study, we found that the confirmed cases of COVID-19 present the
consistent ocular pathological symbols; and we propose a new screening method
of analyzing the eye-region images, captured by common CCD and CMOS cameras,
could reliably make a rapid risk screening of COVID-19 with very high accuracy.
We believe a system implementing such an algorithm should assist the triage
management or the clinical diagnosis. To further evaluate our algorithm and
approved by the Ethics Committee of Shanghai public health clinic center of
Fudan University, we conduct a study of analyzing the eye-region images of 303
patients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136
healthy people. Remarkably, our results of COVID-19 patients in testing set
consistently present similar ocular pathological symbols; and very high testing
results have been achieved in terms of sensitivity and specificity. We hope
this study can be inspiring and helpful for encouraging more researches in this
topic.

---------------

### 21 Feb 2024 | [ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance](https://arxiv.org/abs/2402.13448) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.13448)
*Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong* 

  In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.

---------------

### 13 Jun 2022 | [SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In  Silico Experiments](https://arxiv.org/abs/2206.06127) | [‚¨áÔ∏è](https://arxiv.org/pdf/2206.06127)
*Cong Gao, Benjamin D. Killeen, Yicheng Hu, Robert B. Grupp, Russell H.  Taylor, Mehran Armand, Mathias Unberath* 

  Artificial intelligence (AI) now enables automated interpretation of medical
images for clinical use. However, AI's potential use for interventional images
(versus those involved in triage or diagnosis), such as for guidance during
surgery, remains largely untapped. This is because surgical AI systems are
currently trained using post hoc analysis of data collected during live
surgeries, which has fundamental and practical limitations, including ethical
considerations, expense, scalability, data integrity, and a lack of ground
truth. Here, we demonstrate that creating realistic simulated images from human
models is a viable alternative and complement to large-scale in situ data
collection. We show that training AI image analysis models on realistically
synthesized data, combined with contemporary domain generalization or
adaptation techniques, results in models that on real data perform comparably
to models trained on a precisely matched real data training set. Because
synthetic generation of training data from human-based models scales easily, we
find that our model transfer paradigm for X-ray image analysis, which we refer
to as SyntheX, can even outperform real data-trained models due to the
effectiveness of training on a larger dataset. We demonstrate the potential of
SyntheX on three clinical tasks: Hip image analysis, surgical robotic tool
detection, and COVID-19 lung lesion segmentation. SyntheX provides an
opportunity to drastically accelerate the conception, design, and evaluation of
intelligent systems for X-ray-based medicine. In addition, simulated image
environments provide the opportunity to test novel instrumentation, design
complementary surgical approaches, and envision novel techniques that improve
outcomes, save time, or mitigate human error, freed from the ethical and
practical considerations of live human data collection.

---------------

### 12 May 2022 | [Benchmark datasets driving artificial intelligence development fail to  capture the needs of medical professionals](https://arxiv.org/abs/2201.07040) | [‚¨áÔ∏è](https://arxiv.org/pdf/2201.07040)
*Kathrin Blagec, Jakob Kraiger, Wolfgang Fr\"uhwirt, Matthias Samwald* 

  Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.

---------------

### 26 Jan 2024 | [Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using  Large Language Models to Mitigate Cognitive Bias](https://arxiv.org/abs/2401.14589) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.14589)
*Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal  Abdullah, Daniel Shu Wei Ting, Nan Liu* 

  Background: Cognitive biases in clinical decision-making significantly
contribute to errors in diagnosis and suboptimal patient outcomes. Addressing
these biases presents a formidable challenge in the medical field. This study
explores the role of large language models (LLMs) in mitigating these biases
through the utilization of a multi-agent framework. We simulate the clinical
decision-making processes through multi-agent conversation and evaluate its
efficacy in improving diagnostic accuracy. Methods: A total of 16 published and
unpublished case reports where cognitive biases have resulted in misdiagnoses
were identified from the literature. In the multi-agent system, we leveraged
GPT-4 Turbo to facilitate interactions among four simulated agents to replicate
clinical team dynamics. Each agent has a distinct role: 1) To make the initial
and final diagnosis after considering the discussions, 2) The devil's advocate
and correct confirmation and anchoring bias, 3) The tutor and facilitator of
the discussion to reduce premature closure bias, and 4) To record and summarize
the findings. A total of 80 simulations were evaluated for the accuracy of
initial diagnosis, top differential diagnosis and final two differential
diagnoses. Findings: In a total of 80 responses evaluating both initial and
final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but
following multi-agent discussions, the accuracy for the top differential
diagnosis increased to 71.3% (57/80), and for the final two differential
diagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate
and correct misconceptions, even in scenarios with misleading initial
investigations. Interpretation: The LLM-driven multi-agent conversation system
shows promise in enhancing diagnostic accuracy in diagnostically challenging
medical scenarios.

---------------

### 16 May 2023 | [Towards Expert-Level Medical Question Answering with Large Language  Models](https://arxiv.org/abs/2305.09617) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.09617)
*Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le  Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike  Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant  Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev,  Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral,  Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan  Karthikesalingam, Vivek Natarajan* 

  Recent artificial intelligence (AI) systems have reached milestones in "grand
challenges" ranging from Go to protein-folding. The capability to retrieve
medical knowledge, reason over it, and answer medical questions comparably to
physicians has long been viewed as one such grand challenge.
  Large language models (LLMs) have catalyzed significant progress in medical
question answering; Med-PaLM was the first model to exceed a "passing" score in
US Medical Licensing Examination (USMLE) style questions with a score of 67.2%
on the MedQA dataset. However, this and other prior work suggested significant
room for improvement, especially when models' answers were compared to
clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by
leveraging a combination of base LLM improvements (PaLM 2), medical domain
finetuning, and prompting strategies including a novel ensemble refinement
approach.
  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM
by over 19% and setting a new state-of-the-art. We also observed performance
approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU
clinical topics datasets.
  We performed detailed human evaluations on long-form questions along multiple
axes relevant to clinical applications. In pairwise comparative ranking of 1066
consumer medical questions, physicians preferred Med-PaLM 2 answers to those
produced by physicians on eight of nine axes pertaining to clinical utility (p
< 0.001). We also observed significant improvements compared to Med-PaLM on
every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form
"adversarial" questions to probe LLM limitations.
  While further studies are necessary to validate the efficacy of these models
in real-world settings, these results highlight rapid progress towards
physician-level performance in medical question answering.

---------------

### 13 Feb 2024 | [Leveraging cough sounds to optimize chest x-ray usage in low-resource  settings](https://arxiv.org/abs/2402.08789) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.08789)
*Alexander Philip, Sanya Chawla, Lola Jover, George P. Kafentzis, Joe  Brew, Vishakh Saraf, Shibu Vijayan, Peter Small, Carlos Chaccour* 

  Chest X-ray is a commonly used tool during triage, diagnosis and management
of respiratory diseases. In resource-constricted settings, optimizing this
resource can lead to valuable cost savings for the health care system and the
patients as well as to and improvement in consult time. We used
prospectively-collected data from 137 patients referred for chest X-ray at the
Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each
patient provided at least five coughs while awaiting radiography. Collected
cough sounds were analyzed using acoustic AI methods. Cross-validation was done
on temporal and spectral features on the cough sounds of each patient. Features
were summarized using standard statistical approaches. Three models were
developed, tested and compared in their capacity to predict an abnormal result
in the chest X-ray. All three methods yielded models that could discriminate to
some extent between normal and abnormal with the logistic regression performing
best with an area under the receiver operating characteristic curves ranging
from 0.7 to 0.78. Despite limitations and its relatively small sample size,
this study shows that AI-enabled algorithms can use cough sounds to predict
which individuals presenting for chest radiographic examination will have a
normal or abnormal results. These results call for expanding this research
given the potential optimization of limited health care resources in low- and
middle-income countries.

---------------

### 22 Oct 2019 | [Who wants accurate models? Arguing for a different metrics to take  classification models seriously](https://arxiv.org/abs/1910.09246) | [‚¨áÔ∏è](https://arxiv.org/pdf/1910.09246)
*Federico Cabitza and Andrea Campagner* 

  With the increasing availability of AI-based decision support, there is an
increasing need for their certification by both AI manufacturers and notified
bodies, as well as the pragmatic (real-world) validation of these systems.
Therefore, there is the need for meaningful and informative ways to assess the
performance of AI systems in clinical practice. Common metrics (like accuracy
scores and areas under the ROC curve) have known problems and they do not take
into account important information about the preferences of clinicians and the
needs of their specialist practice, like the likelihood and impact of errors
and the complexity of cases. In this paper, we present a new accuracy measure,
the H-accuracy (Ha), which we claim is more informative in the medical domain
(and others of similar needs) for the elements it encompasses. We also provide
proof that the H-accuracy is a generalization of the balanced accuracy and
establish a relation between the H-accuracy and the Net Benefit. Finally, we
illustrate an experimentation in two user studies to show the descriptive power
of the Ha score and how complementary and differently informative measures can
be derived from its formulation (a Python script to compute Ha is also made
available).

---------------
**Date:** 27 Jun 2018

**Title:** A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis

**Abstract Link:** [https://arxiv.org/abs/1806.10698](https://arxiv.org/abs/1806.10698)

**PDF Link:** [https://arxiv.org/pdf/1806.10698](https://arxiv.org/pdf/1806.10698)

---

**Date:** 09 May 2023

**Title:** The Case Records of ChatGPT: Language Models and Complex Clinical  Questions

**Abstract Link:** [https://arxiv.org/abs/2305.05609](https://arxiv.org/abs/2305.05609)

**PDF Link:** [https://arxiv.org/pdf/2305.05609](https://arxiv.org/pdf/2305.05609)

---

**Date:** 28 May 2021

**Title:** Can artificial intelligence (AI) be used to accurately detect  tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB  screening and triaging in a high TB burden setting

**Abstract Link:** [https://arxiv.org/abs/2006.05509](https://arxiv.org/abs/2006.05509)

**PDF Link:** [https://arxiv.org/pdf/2006.05509](https://arxiv.org/pdf/2006.05509)

---

**Date:** 11 May 2022

**Title:** Performance of a deep learning system for detection of referable  diabetic retinopathy in real clinical settings

**Abstract Link:** [https://arxiv.org/abs/2205.05554](https://arxiv.org/abs/2205.05554)

**PDF Link:** [https://arxiv.org/pdf/2205.05554](https://arxiv.org/pdf/2205.05554)

---

**Date:** 18 Feb 2022

**Title:** An Integrated Optimization and Machine Learning Models to Predict the  Admission Status of Emergency Patients

**Abstract Link:** [https://arxiv.org/abs/2202.09196](https://arxiv.org/abs/2202.09196)

**PDF Link:** [https://arxiv.org/pdf/2202.09196](https://arxiv.org/pdf/2202.09196)

---

**Date:** 07 Jan 2022

**Title:** Monitoring Covid-19 on social media using a novel triage and diagnosis  approach

**Abstract Link:** [https://arxiv.org/abs/2103.11850](https://arxiv.org/abs/2103.11850)

**PDF Link:** [https://arxiv.org/pdf/2103.11850](https://arxiv.org/pdf/2103.11850)

---

**Date:** 29 Jun 2023

**Title:** Improving Patient Pre-screening for Clinical Trials: Assisting  Physicians with Large Language Models

**Abstract Link:** [https://arxiv.org/abs/2304.07396](https://arxiv.org/abs/2304.07396)

**PDF Link:** [https://arxiv.org/pdf/2304.07396](https://arxiv.org/pdf/2304.07396)

---

**Date:** 14 Jan 2024

**Title:** Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by  Integrating Expert Knowledge and Interpretable Data-driven Intelligence

**Abstract Link:** [https://arxiv.org/abs/2401.08695](https://arxiv.org/abs/2401.08695)

**PDF Link:** [https://arxiv.org/pdf/2401.08695](https://arxiv.org/pdf/2401.08695)

---

**Date:** 28 Jun 2021

**Title:** Using machine learning techniques to predict hospital admission at the  emergency department

**Abstract Link:** [https://arxiv.org/abs/2106.12921](https://arxiv.org/abs/2106.12921)

**PDF Link:** [https://arxiv.org/pdf/2106.12921](https://arxiv.org/pdf/2106.12921)

---

**Date:** 13 Sep 2018

**Title:** Focus Group on Artificial Intelligence for Health

**Abstract Link:** [https://arxiv.org/abs/1809.04797](https://arxiv.org/abs/1809.04797)

**PDF Link:** [https://arxiv.org/pdf/1809.04797](https://arxiv.org/pdf/1809.04797)

---

**Date:** 31 Aug 2022

**Title:** Robustness of an Artificial Intelligence Solution for Diagnosis of  Normal Chest X-Rays

**Abstract Link:** [https://arxiv.org/abs/2209.09204](https://arxiv.org/abs/2209.09204)

**PDF Link:** [https://arxiv.org/pdf/2209.09204](https://arxiv.org/pdf/2209.09204)

---

**Date:** 11 Dec 2023

**Title:** The Limits of Fair Medical Imaging AI In The Wild

**Abstract Link:** [https://arxiv.org/abs/2312.10083](https://arxiv.org/abs/2312.10083)

**PDF Link:** [https://arxiv.org/pdf/2312.10083](https://arxiv.org/pdf/2312.10083)

---

**Date:** 04 Sep 2020

**Title:** A New Screening Method for COVID-19 based on Ocular Feature Recognition  by Machine Learning Tools

**Abstract Link:** [https://arxiv.org/abs/2009.03184](https://arxiv.org/abs/2009.03184)

**PDF Link:** [https://arxiv.org/pdf/2009.03184](https://arxiv.org/pdf/2009.03184)

---

**Date:** 21 Feb 2024

**Title:** ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance

**Abstract Link:** [https://arxiv.org/abs/2402.13448](https://arxiv.org/abs/2402.13448)

**PDF Link:** [https://arxiv.org/pdf/2402.13448](https://arxiv.org/pdf/2402.13448)

---

**Date:** 13 Jun 2022

**Title:** SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In  Silico Experiments

**Abstract Link:** [https://arxiv.org/abs/2206.06127](https://arxiv.org/abs/2206.06127)

**PDF Link:** [https://arxiv.org/pdf/2206.06127](https://arxiv.org/pdf/2206.06127)

---

**Date:** 12 May 2022

**Title:** Benchmark datasets driving artificial intelligence development fail to  capture the needs of medical professionals

**Abstract Link:** [https://arxiv.org/abs/2201.07040](https://arxiv.org/abs/2201.07040)

**PDF Link:** [https://arxiv.org/pdf/2201.07040](https://arxiv.org/pdf/2201.07040)

---

**Date:** 26 Jan 2024

**Title:** Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using  Large Language Models to Mitigate Cognitive Bias

**Abstract Link:** [https://arxiv.org/abs/2401.14589](https://arxiv.org/abs/2401.14589)

**PDF Link:** [https://arxiv.org/pdf/2401.14589](https://arxiv.org/pdf/2401.14589)

---

**Date:** 16 May 2023

**Title:** Towards Expert-Level Medical Question Answering with Large Language  Models

**Abstract Link:** [https://arxiv.org/abs/2305.09617](https://arxiv.org/abs/2305.09617)

**PDF Link:** [https://arxiv.org/pdf/2305.09617](https://arxiv.org/pdf/2305.09617)

---

**Date:** 13 Feb 2024

**Title:** Leveraging cough sounds to optimize chest x-ray usage in low-resource  settings

**Abstract Link:** [https://arxiv.org/abs/2402.08789](https://arxiv.org/abs/2402.08789)

**PDF Link:** [https://arxiv.org/pdf/2402.08789](https://arxiv.org/pdf/2402.08789)

---

**Date:** 22 Oct 2019

**Title:** Who wants accurate models? Arguing for a different metrics to take  classification models seriously

**Abstract Link:** [https://arxiv.org/abs/1910.09246](https://arxiv.org/abs/1910.09246)

**PDF Link:** [https://arxiv.org/pdf/1910.09246](https://arxiv.org/pdf/1910.09246)

---

in ai-led medical interviews we collect initial evidence such as symptoms risk factors pre-existing conditions I existing provider details and then also patient demographic data\n with intelligent interviews and drawing conclusions to be able to make triage recommendations we require that we have human in the loop testing and also evaluation of the accuracy of differential diagnosis outcomes along with patient education information what is the state of the art of understanding the performance metrics the bias and fairness and also AI safety testing to be able to do this accurately and in an environment with clinical reviews with medical professionals being able to evaluate and provide feedback on conditions symptoms risk factors and the outcome data\n

### üîé in ai-led medical interviews we collect initial evidence such as symptoms risk factors pre-existing conditions I existing provider details and then also patient demographic data\n with intelligent interviews and drawing conclusions to be able to make triage recommendations we require that we have human in the loop testing and also evaluation of the accuracy of differential diagnosis outcomes along with patient education information what is the state of the art of understanding the performance metrics the bias and fairness and also AI safety testing to be able to do this accurately and in an environment with clinical reviews with medical professionals being able to evaluate and provide feedback on conditions symptoms risk factors and the outcome data\n

 what are the best practices for AI safety testing and evaluation of AI models in the medical domain?

The state of the art in understanding the performance metrics, bias, fairness, and AI safety testing in the medical domain is still evolving. However, there are some best practices that can be followed to ensure the safe and effective use of AI in medical interviews and diagnosis.

1. Human-in-the-loop testing: It is important to have human experts in the loop during the development and testing of AI models for medical interviews. This can help ensure that the AI model is making accurate and safe recommendations, and can also help identify any potential biases or errors in the model.
2. Accuracy and precision: The accuracy and precision of the AI model should be evaluated using appropriate metrics, such as sensitivity, specificity, and positive predictive value. These metrics should be calculated using a diverse and representative dataset to ensure that the model performs well across different populations.
3. Bias and fairness: Bias and fairness should be evaluated by testing the AI model on a diverse and representative dataset. It is important to ensure that the model does not discriminate against certain groups or populations, and that it performs equally well across different demographics.
4. Explainability: The AI model should be explainable, meaning that it should be possible to understand how the model is making its recommendations. This is important for building trust with patients and healthcare providers, and for ensuring that the model is making safe and appropriate recommendations.
5. Clinical reviews: The AI model should be reviewed by medical professionals to ensure that the recommendations are clinically sound and safe. This can help identify any potential issues or concerns with the model, and can also provide valuable feedback for improving the model.
6. Patient education: It is important to provide patients with education and resources to help them understand their condition and treatment options. This can help ensure that patients are informed and empowered to make decisions about their healthcare.
7. Continuous monitoring: The AI model should be continuously monitored and evaluated to ensure that it is performing accurately and safely. This can help identify any potential issues or concerns, and can also provide valuable feedback for improving the model.

Overall, the safe and effective use of AI in medical interviews and diagnosis requires a multifaceted approach that includes human-in-the-loop testing, accuracy and precision evaluation, bias and fairness assessment, explainability, clinical reviews
# ü©∫üîç Search Results
### 18 Jan 2022 | [Label-dependent and event-guided interpretable disease risk prediction  using EHRs](https://arxiv.org/abs/2201.06783) | [‚¨áÔ∏è](https://arxiv.org/pdf/2201.06783)
*Shuai Niu and Yunya Song and Qing Yin and Yike Guo and Xian Yang* 

  Electronic health records (EHRs) contain patients' heterogeneous data that
are collected from medical providers involved in the patient's care, including
medical notes, clinical events, laboratory test results, symptoms, and
diagnoses. In the field of modern healthcare, predicting whether patients would
experience any risks based on their EHRs has emerged as a promising research
area, in which artificial intelligence (AI) plays a key role. To make AI models
practically applicable, it is required that the prediction results should be
both accurate and interpretable. To achieve this goal, this paper proposed a
label-dependent and event-guided risk prediction model (LERP) to predict the
presence of multiple disease risks by mainly extracting information from
unstructured medical notes. Our model is featured in the following aspects.
First, we adopt a label-dependent mechanism that gives greater attention to
words from medical notes that are semantically similar to the names of risk
labels. Secondly, as the clinical events (e.g., treatments and drugs) can also
indicate the health status of patients, our model utilizes the information from
events and uses them to generate an event-guided representation of medical
notes. Thirdly, both label-dependent and event-guided representations are
integrated to make a robust prediction, in which the interpretability is
enabled by the attention weights over words from medical notes. To demonstrate
the applicability of the proposed method, we apply it to the MIMIC-III dataset,
which contains real-world EHRs collected from hospitals. Our method is
evaluated in both quantitative and qualitative ways.

---------------

### 11 Feb 2023 | [Informing clinical assessment by contextualizing post-hoc explanations  of risk prediction models in type-2 diabetes](https://arxiv.org/abs/2302.05752) | [‚¨áÔ∏è](https://arxiv.org/pdf/2302.05752)
*Shruthi Chari, Prasant Acharya, Daniel M. Gruen, Olivia Zhang, Elif K.  Eyigoz, Mohamed Ghalwash, Oshani Seneviratne, Fernando Suarez Saiz, Pablo  Meyer, Prithwish Chakraborty, Deborah L. McGuinness* 

  Medical experts may use Artificial Intelligence (AI) systems with greater
trust if these are supported by contextual explanations that let the
practitioner connect system inferences to their context of use. However, their
importance in improving model usage and understanding has not been extensively
studied. Hence, we consider a comorbidity risk prediction scenario and focus on
contexts regarding the patients clinical state, AI predictions about their risk
of complications, and algorithmic explanations supporting the predictions. We
explore how relevant information for such dimensions can be extracted from
Medical guidelines to answer typical questions from clinical practitioners. We
identify this as a question answering (QA) task and employ several
state-of-the-art LLMs to present contexts around risk prediction model
inferences and evaluate their acceptability. Finally, we study the benefits of
contextual explanations by building an end-to-end AI pipeline including data
cohorting, AI risk modeling, post-hoc model explanations, and prototyped a
visual dashboard to present the combined insights from different context
dimensions and data sources, while predicting and identifying the drivers of
risk of Chronic Kidney Disease - a common type-2 diabetes comorbidity. All of
these steps were performed in engagement with medical experts, including a
final evaluation of the dashboard results by an expert medical panel. We show
that LLMs, in particular BERT and SciBERT, can be readily deployed to extract
some relevant explanations to support clinical usage. To understand the
value-add of the contextual explanations, the expert panel evaluated these
regarding actionable insights in the relevant clinical setting. Overall, our
paper is one of the first end-to-end analyses identifying the feasibility and
benefits of contextual explanations in a real-world clinical use case.

---------------

### 27 Jun 2018 | [A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis](https://arxiv.org/abs/1806.10698) | [‚¨áÔ∏è](https://arxiv.org/pdf/1806.10698)
*Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri* 

  Online symptom checkers have significant potential to improve patient care,
however their reliability and accuracy remain variable. We hypothesised that an
artificial intelligence (AI) powered triage and diagnostic system would compare
favourably with human doctors with respect to triage and diagnostic accuracy.
We performed a prospective validation study of the accuracy and safety of an AI
powered triage and diagnostic system. Identical cases were evaluated by both an
AI system and human doctors. Differential diagnoses and triage outcomes were
evaluated by an independent judge, who was blinded from knowing the source (AI
system or human doctor) of the outcomes. Independently of these cases,
vignettes from publicly available resources were also assessed to provide a
benchmark to previous studies and the diagnostic component of the MRCGP exam.
Overall we found that the Babylon AI powered Triage and Diagnostic System was
able to identify the condition modelled by a clinical vignette with accuracy
comparable to human doctors (in terms of precision and recall). In addition, we
found that the triage advice recommended by the AI System was, on average,
safer than that of human doctors, when compared to the ranges of acceptable
triage provided by independent expert judges, with only a minimal reduction in
appropriateness.

---------------

### 15 Jul 2021 | [Leveraging Clinical Context for User-Centered Explainability: A Diabetes  Use Case](https://arxiv.org/abs/2107.02359) | [‚¨áÔ∏è](https://arxiv.org/pdf/2107.02359)
*Shruthi Chari, Prithwish Chakraborty, Mohamed Ghalwash, Oshani  Seneviratne, Elif K. Eyigoz, Daniel M. Gruen, Fernando Suarez Saiz, Ching-Hua  Chen, Pablo Meyer Rojas, Deborah L. McGuinness* 

  Academic advances of AI models in high-precision domains, like healthcare,
need to be made explainable in order to enhance real-world adoption. Our past
studies and ongoing interactions indicate that medical experts can use AI
systems with greater trust if there are ways to connect the model inferences
about patients to explanations that are tied back to the context of use.
Specifically, risk prediction is a complex problem of diagnostic and
interventional importance to clinicians wherein they consult different sources
to make decisions. To enable the adoption of the ever improving AI risk
prediction models in practice, we have begun to explore techniques to
contextualize such models along three dimensions of interest: the patients'
clinical state, AI predictions about their risk of complications, and
algorithmic explanations supporting the predictions. We validate the importance
of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes
(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a
common T2DM comorbidity. Within the POC, we include risk prediction models for
CKD, post-hoc explainers of the predictions, and other natural-language modules
which operationalize domain knowledge and CPGs to provide context. With primary
care physicians (PCP) as our end-users, we present our initial results and
clinician feedback in this paper. Our POC approach covers multiple knowledge
sources and clinical scenarios, blends knowledge to explain data and
predictions to PCPs, and received an enthusiastic response from our medical
expert.

---------------

### 08 Feb 2021 | [Clinical Outcome Prediction from Admission Notes using Self-Supervised  Knowledge Integration](https://arxiv.org/abs/2102.04110) | [‚¨áÔ∏è](https://arxiv.org/pdf/2102.04110)
*Betty van Aken, Jens-Michalis Papaioannou, Manuel Mayrdorfer, Klemens  Budde, Felix A. Gers, Alexander L\"oser* 

  Outcome prediction from clinical text can prevent doctors from overlooking
possible risks and help hospitals to plan capacities. We simulate patients at
admission time, when decision support can be especially valuable, and
contribute a novel admission to discharge task with four common outcome
prediction targets: Diagnoses at discharge, procedures performed, in-hospital
mortality and length-of-stay prediction. The ideal system should infer outcomes
based on symptoms, pre-conditions and risk factors of a patient. We evaluate
the effectiveness of language models to handle this scenario and propose
clinical outcome pre-training to integrate knowledge about patient outcomes
from multiple public sources. We further present a simple method to incorporate
ICD code hierarchy into the models. We show that our approach improves
performance on the outcome tasks against several baselines. A detailed analysis
reveals further strengths of the model, including transferability, but also
weaknesses such as handling of vital values and inconsistencies in the
underlying data.

---------------

### 02 Aug 2019 | [Mixed-Integer Optimization Approach to Learning Association Rules for  Unplanned ICU Transfer](https://arxiv.org/abs/1908.00966) | [‚¨áÔ∏è](https://arxiv.org/pdf/1908.00966)
*Chun-An Chou and Qingtao Cao and Shao-Jen Weng and Che-Hung Tsai* 

  After admission to emergency department (ED), patients with critical
illnesses are transferred to intensive care unit (ICU) due to unexpected
clinical deterioration occurrence. Identifying such unplanned ICU transfers is
urgently needed for medical physicians to achieve two-fold goals: improving
critical care quality and preventing mortality. A priority task is to
understand the crucial rationale behind diagnosis results of individual
patients during stay in ED, which helps prepare for an early transfer to ICU.
Most existing prediction studies were based on univariate analysis or multiple
logistic regression to provide one-size-fit-all results. However, patient
condition varying from case to case may not be accurately examined by the only
judgment. In this study, we present a new decision tool using a mathematical
optimization approach aiming to automatically discover rules associating
diagnostic features with high-risk outcome (i.e., unplanned transfers) in
different deterioration scenarios. We consider four mutually exclusive patient
subgroups based on the principal reasons of ED visits: infections,
cardiovascular/respiratory diseases, gastrointestinal diseases, and
neurological/other diseases at a suburban teaching hospital. The analysis
results demonstrate significant rules associated with unplanned transfer
outcome for each subgroups and also show comparable prediction accuracy,
compared to state-of-the-art machine learning methods while providing
easy-to-interpret symptom-outcome information.

---------------

### 21 Feb 2024 | [ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance](https://arxiv.org/abs/2402.13448) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.13448)
*Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong* 

  In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.

---------------

### 16 Aug 2023 | [Explainable AI for clinical risk prediction: a survey of concepts,  methods, and modalities](https://arxiv.org/abs/2308.08407) | [‚¨áÔ∏è](https://arxiv.org/pdf/2308.08407)
*Munib Mesinovic, Peter Watkinson, Tingting Zhu* 

  Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

---------------

### 06 Dec 2023 | [Domain constraints improve risk prediction when outcome data is missing](https://arxiv.org/abs/2312.03878) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.03878)
*Sidhika Balachandar, Nikhil Garg, Emma Pierson* 

  Machine learning models are often trained to predict the outcome resulting
from a human decision. For example, if a doctor decides to test a patient for
disease, will the patient test positive? A challenge is that the human decision
censors the outcome data: we only observe test outcomes for patients doctors
historically tested. Untested patients, for whom outcomes are unobserved, may
differ from tested patients along observed and unobserved dimensions. We
propose a Bayesian model class which captures this setting. The purpose of the
model is to accurately estimate risk for both tested and untested patients.
Estimating this model is challenging due to the wide range of possibilities for
untested patients. To address this, we propose two domain constraints which are
plausible in health settings: a prevalence constraint, where the overall
disease prevalence is known, and an expertise constraint, where the human
decision-maker deviates from purely risk-based decision-making only along a
constrained feature set. We show theoretically and on synthetic data that
domain constraints improve parameter inference. We apply our model to a case
study of cancer risk prediction, showing that the model's inferred risk
predicts cancer diagnoses, its inferred testing policy captures known public
health policies, and it can identify suboptimalities in test allocation. Though
our case study is in healthcare, our analysis reveals a general class of domain
constraints which can improve model estimation in many settings.

---------------

### 11 Aug 2023 | [ExBEHRT: Extended Transformer for Electronic Health Records to Predict  Disease Subtypes & Progressions](https://arxiv.org/abs/2303.12364) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.12364)
*Maurice Rupp, Oriane Peter, Thirupathi Pattipaka* 

  In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT
applied to electronic health records), and apply different algorithms to
interpret its results. While BEHRT considers only diagnoses and patient age, we
extend the feature space to several multimodal records, namely demographics,
clinical characteristics, vital signs, smoking status, diagnoses, procedures,
medications, and laboratory tests, by applying a novel method to unify the
frequencies and temporal dimensions of the different features. We show that
additional features significantly improve model performance for various
downstream tasks in different diseases. To ensure robustness, we interpret
model predictions using an adaptation of expected gradients, which has not been
previously applied to transformers with EHR data and provides more granular
interpretations than previous approaches such as feature and token importances.
Furthermore, by clustering the model representations of oncology patients, we
show that the model has an implicit understanding of the disease and is able to
classify patients with the same cancer type into different risk groups. Given
the additional features and interpretability, ExBEHRT can help make informed
decisions about disease trajectories, diagnoses, and risk factors of various
diseases.

---------------

### 15 Feb 2024 | [Towards Reducing Diagnostic Errors with Interpretable Risk Prediction](https://arxiv.org/abs/2402.10109) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.10109)
*Denis Jered McInerney, William Dickinson, Lucy Flynn, Andrea Young,  Geoffrey Young, Jan-Willem van de Meent, Byron C. Wallace* 

  Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.

---------------

### 15 Oct 2021 | [A New Approach for Interpretability and Reliability in Clinical Risk  Prediction: Acute Coronary Syndrome Scenario](https://arxiv.org/abs/2110.08331) | [‚¨áÔ∏è](https://arxiv.org/pdf/2110.08331)
*Francisco Valente, Jorge Henriques, Sim\~ao Paredes, Teresa Rocha,  Paulo de Carvalho, Jo\~ao Morais* 

  We intend to create a new risk assessment methodology that combines the best
characteristics of both risk score and machine learning models. More
specifically, we aim to develop a method that, besides having a good
performance, offers a personalized model and outcome for each patient, presents
high interpretability, and incorporates an estimation of the prediction
reliability which is not usually available. By combining these features in the
same approach we expect that it can boost the confidence of physicians to use
such a tool in their daily activity. In order to achieve the mentioned goals, a
three-step methodology was developed: several rules were created by
dichotomizing risk factors; such rules were trained with a machine learning
classifier to predict the acceptance degree of each rule (the probability that
the rule is correct) for each patient; that information was combined and used
to compute the risk of mortality and the reliability of such prediction. The
methodology was applied to a dataset of patients admitted with any type of
acute coronary syndromes (ACS), to assess the 30-days all-cause mortality risk.
The performance was compared with state-of-the-art approaches: logistic
regression (LR), artificial neural network (ANN), and clinical risk score model
(Global Registry of Acute Coronary Events - GRACE). The proposed approach
achieved testing results identical to the standard LR, but offers superior
interpretability and personalization; it also significantly outperforms the
GRACE risk model and the standard ANN model. The calibration curve also
suggests a very good generalization ability of the obtained model as it
approaches the ideal curve. Finally, the reliability estimation of individual
predictions presented a great correlation with the misclassifications rate.
Those properties may have a beneficial application in other clinical scenarios
as well. [abridged]

---------------

### 26 Feb 2024 | [Rethinking Human-AI Collaboration in Complex Medical Decision Making: A  Case Study in Sepsis Diagnosis](https://arxiv.org/abs/2309.12368) | [‚¨áÔ∏è](https://arxiv.org/pdf/2309.12368)
*Shao Zhang, Jianing Yu, Xuhai Xu, Changchang Yin, Yuxuan Lu, Bingsheng  Yao, Melanie Tory, Lace M. Padilla, Jeffrey Caterino, Ping Zhang, Dakuo Wang* 

  Today's AI systems for medical decision support often succeed on benchmark
datasets in research papers but fail in real-world deployment. This work
focuses on the decision making of sepsis, an acute life-threatening systematic
infection that requires an early diagnosis with high uncertainty from the
clinician. Our aim is to explore the design requirements for AI systems that
can support clinical experts in making better decisions for the early diagnosis
of sepsis. The study begins with a formative study investigating why clinical
experts abandon an existing AI-powered Sepsis predictive module in their
electrical health record (EHR) system. We argue that a human-centered AI system
needs to support human experts in the intermediate stages of a medical
decision-making process (e.g., generating hypotheses or gathering data),
instead of focusing only on the final decision. Therefore, we build SepsisLab
based on a state-of-the-art AI algorithm and extend it to predict the future
projection of sepsis development, visualize the prediction uncertainty, and
propose actionable suggestions (i.e., which additional laboratory tests can be
collected) to reduce such uncertainty. Through heuristic evaluation with six
clinicians using our prototype system, we demonstrate that SepsisLab enables a
promising human-AI collaboration paradigm for the future of AI-assisted sepsis
diagnosis and other high-stakes medical decision making.

---------------

### 29 Jan 2024 | [Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation  for Automatic Diagnosis](https://arxiv.org/abs/2401.16107) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.16107)
*Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, Ting Liu* 

  Automatic diagnosis is a significant application of AI in healthcare, where
diagnoses are generated based on the symptom description of patients. Previous
works have approached this task directly by modeling the relationship between
the normalized symptoms and all possible diseases. However, in the clinical
diagnostic process, patients are initially consulted by a general practitioner
and, if necessary, referred to specialists in specific domains for a more
comprehensive evaluation. The final diagnosis often emerges from a
collaborative consultation among medical specialist groups. Recently, large
language models have shown impressive capabilities in natural language
understanding. In this study, we adopt tuning-free LLM-based agents as medical
practitioners and propose the Agent-derived Multi-Specialist Consultation
(AMSC) framework to model the diagnosis process in the real world by adaptively
fusing probability distributions of agents over potential diseases.
Experimental results demonstrate the superiority of our approach compared with
baselines. Notably, our approach requires significantly less parameter updating
and training time, enhancing efficiency and practical utility. Furthermore, we
delve into a novel perspective on the role of implicit symptoms within the
context of automatic diagnosis.

---------------

### 07 Feb 2023 | [An Expert System to Diagnose Spinal Disorders](https://arxiv.org/abs/2302.03625) | [‚¨áÔ∏è](https://arxiv.org/pdf/2302.03625)
*Seyed Mohammad Sadegh Dashti, Seyedeh Fatemeh Dashti* 

  Objective: Until now, traditional invasive approaches have been the only
means being leveraged to diagnose spinal disorders. Traditional manual
diagnostics require a high workload, and diagnostic errors are likely to occur
due to the prolonged work of physicians. In this research, we develop an expert
system based on a hybrid inference algorithm and comprehensive integrated
knowledge for assisting the experts in the fast and high-quality diagnosis of
spinal disorders.
  Methods: First, for each spinal anomaly, the accurate and integrated
knowledge was acquired from related experts and resources. Second, based on
probability distributions and dependencies between symptoms of each anomaly, a
unique numerical value known as certainty effect value was assigned to each
symptom. Third, a new hybrid inference algorithm was designed to obtain
excellent performance, which was an incorporation of the Backward Chaining
Inference and Theory of Uncertainty.
  Results: The proposed expert system was evaluated in two different phases,
real-world samples, and medical records evaluation. Evaluations show that in
terms of real-world samples analysis, the system achieved excellent accuracy.
Application of the system on the sample with anomalies revealed the degree of
severity of disorders and the risk of development of abnormalities in unhealthy
and healthy patients. In the case of medical records analysis, our expert
system proved to have promising performance, which was very close to those of
experts.
  Conclusion: Evaluations suggest that the proposed expert system provides
promising performance, helping specialists to validate the accuracy and
integrity of their diagnosis. It can also serve as an intelligent educational
software for medical students to gain familiarity with spinal disorder
diagnosis process, and related symptoms.

---------------

### 27 Nov 2019 | [AdaCare: Explainable Clinical Health Status Representation Learning via  Scale-Adaptive Feature Extraction and Recalibration](https://arxiv.org/abs/1911.12205) | [‚¨áÔ∏è](https://arxiv.org/pdf/1911.12205)
*Liantao Ma, Junyi Gao, Yasha Wang, Chaohe Zhang, Jiangtao Wang, Wenjie  Ruan, Wen Tang, Xin Gao, Xinyu Ma* 

  Deep learning-based health status representation learning and clinical
prediction have raised much research interest in recent years. Existing models
have shown superior performance, but there are still several major issues that
have not been fully taken into consideration. First, the historical variation
pattern of the biomarker in diverse time scales plays a vital role in
indicating the health status, but it has not been explicitly extracted by
existing works. Second, key factors that strongly indicate the health risk are
different among patients. It is still challenging to adaptively make use of the
features for patients in diverse conditions. Third, using prediction models as
the black box will limit the reliability in clinical practice. However, none of
the existing works can provide satisfying interpretability and meanwhile
achieve high prediction performance. In this work, we develop a general health
status representation learning model, named AdaCare. It can capture the long
and short-term variations of biomarkers as clinical features to depict the
health status in multiple time scales. It also models the correlation between
clinical features to enhance the ones which strongly indicate the health status
and thus can maintain a state-of-the-art performance in terms of prediction
accuracy while providing qualitative interpretability. We conduct a health risk
prediction experiment on two real-world datasets. Experiment results indicate
that AdaCare outperforms state-of-the-art approaches and provides effective
interpretability, which is verifiable by clinical experts.

---------------

### 28 Nov 2018 | [Disease phenotyping using deep learning: A diabetes case study](https://arxiv.org/abs/1811.11818) | [‚¨áÔ∏è](https://arxiv.org/pdf/1811.11818)
*Sina Rashidian, Janos Hajagos, Richard Moffitt, Fusheng Wang, Xinyu  Dong, Kayley Abell-Hart, Kimberly Noel, Rajarsi Gupta, Mathew Tharakan, Veena  Lingam, Joel Saltz, Mary Saltz* 

  Characterization of a patient clinical phenotype is central to biomedical
informatics. ICD codes, assigned to inpatient encounters by coders, is
important for population health and cohort discovery when clinical information
is limited. While ICD codes are assigned to patients by professionals trained
and certified in coding there is substantial variability in coding. We present
a methodology that uses deep learning methods to model coder decision making
and that predicts ICD codes. Our approach predicts codes based on demographics,
lab results, and medications, as well as codes from previous encounters. We are
able to predict existing codes with high accuracy for all three of the test
cases we investigated: diabetes, acute renal failure, and chronic kidney
disease. We employed a panel of clinicians, in a blinded manner, to assess
ground truth and compared the predictions of coders, model and clinicians. When
disparities between the model prediction and coder assigned codes were
reviewed, our model outperformed coder assigned ICD codes.

---------------

### 27 Oct 2023 | [Auditing for Human Expertise](https://arxiv.org/abs/2306.01646) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.01646)
*Rohan Alur, Loren Laine, Darrick K. Li, Manish Raghavan, Devavrat  Shah, Dennis Shung* 

  High-stakes prediction tasks (e.g., patient diagnosis) are often handled by
trained human experts. A common source of concern about automation in these
settings is that experts may exercise intuition that is difficult to model
and/or have access to information (e.g., conversations with a patient) that is
simply unavailable to a would-be algorithm. This raises a natural question
whether human experts add value which could not be captured by an algorithmic
predictor. We develop a statistical framework under which we can pose this
question as a natural hypothesis test. Indeed, as our framework highlights,
detecting human expertise is more subtle than simply comparing the accuracy of
expert predictions to those made by a particular learning algorithm. Instead,
we propose a simple procedure which tests whether expert predictions are
statistically independent from the outcomes of interest after conditioning on
the available inputs (`features'). A rejection of our test thus suggests that
human experts may add value to any algorithm trained on the available data, and
has direct implications for whether human-AI `complementarity' is achievable in
a given prediction task. We highlight the utility of our procedure using
admissions data collected from the emergency department of a large academic
hospital system, where we show that physicians' admit/discharge decisions for
patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating
information that is not available to a standard algorithmic screening tool.
This is despite the fact that the screening tool is arguably more accurate than
physicians' discretionary decisions, highlighting that -- even absent normative
concerns about accountability or interpretability -- accuracy is insufficient
to justify algorithmic automation.

---------------

### 09 Oct 2023 | [RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19  Assessment in Primary Care](https://arxiv.org/abs/2306.17175) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.17175)
*Rakhilya Lee Mekhtieva, Brandon Forbes, Dalal Alrajeh, Brendan  Delaney, Alessandra Russo* 

  Clinical decision-making is a fundamental stage in delivering appropriate
care to patients. In recent years several decision-making systems designed to
aid the clinician in this process have been developed. However, technical
solutions currently in use are based on simple regression models and are only
able to take into account simple pre-defined multiple-choice features, such as
patient age, pre-existing conditions, smoker status, etc. One particular source
of patient data, that available decision-making systems are incapable of
processing is the collection of patient consultation GP notes. These contain
crucial signs and symptoms - the information used by clinicians in order to
make a final decision and direct the patient to the appropriate care.
Extracting information from GP notes is a technically challenging problem, as
they tend to include abbreviations, typos, and incomplete sentences.
  This paper addresses this open challenge. We present a framework that
performs knowledge graph construction from raw GP medical notes written during
or after patient consultations. By relying on support phrases mined from the
SNOMED ontology, as well as predefined supported facts from values used in the
RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction
tool, our graph generative framework is able to extract structured knowledge
graphs from the highly unstructured and inconsistent format that consultation
notes are written in. Our knowledge graphs include information about existing
patient symptoms, their duration, and their severity.
  We apply our framework to consultation notes of COVID-19 patients in the UK
COVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a
quantitative evaluation of the performance of our framework, demonstrating that
our approach has better accuracy than traditional NLP methods when answering
questions about patients.

---------------

### 14 May 2021 | [A causal learning framework for the analysis and interpretation of  COVID-19 clinical data](https://arxiv.org/abs/2105.06998) | [‚¨áÔ∏è](https://arxiv.org/pdf/2105.06998)
*Elisa Ferrari, Luna Gargani, Greta Barbieri, Lorenzo Ghiadoni,  Francesco Faita, Davide Bacciu* 

  We present a workflow for clinical data analysis that relies on Bayesian
Structure Learning (BSL), an unsupervised learning approach, robust to noise
and biases, that allows to incorporate prior medical knowledge into the
learning process and that provides explainable results in the form of a graph
showing the causal connections among the analyzed features. The workflow
consists in a multi-step approach that goes from identifying the main causes of
patient's outcome through BSL, to the realization of a tool suitable for
clinical practice, based on a Binary Decision Tree (BDT), to recognize patients
at high-risk with information available already at hospital admission time. We
evaluate our approach on a feature-rich COVID-19 dataset, showing that the
proposed framework provides a schematic overview of the multi-factorial
processes that jointly contribute to the outcome. We discuss how these
computational findings are confirmed by current understanding of the COVID-19
pathogenesis. Further, our approach yields to a highly interpretable tool
correctly predicting the outcome of 85% of subjects based exclusively on 3
features: age, a previous history of chronic obstructive pulmonary disease and
the PaO2/FiO2 ratio at the time of arrival to the hospital. The inclusion of
additional information from 4 routine blood tests (Creatinine, Glucose, pO2 and
Sodium) increases predictive accuracy to 94.5%.

---------------
**Date:** 18 Jan 2022

**Title:** Label-dependent and event-guided interpretable disease risk prediction  using EHRs

**Abstract Link:** [https://arxiv.org/abs/2201.06783](https://arxiv.org/abs/2201.06783)

**PDF Link:** [https://arxiv.org/pdf/2201.06783](https://arxiv.org/pdf/2201.06783)

---

**Date:** 11 Feb 2023

**Title:** Informing clinical assessment by contextualizing post-hoc explanations  of risk prediction models in type-2 diabetes

**Abstract Link:** [https://arxiv.org/abs/2302.05752](https://arxiv.org/abs/2302.05752)

**PDF Link:** [https://arxiv.org/pdf/2302.05752](https://arxiv.org/pdf/2302.05752)

---

**Date:** 27 Jun 2018

**Title:** A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis

**Abstract Link:** [https://arxiv.org/abs/1806.10698](https://arxiv.org/abs/1806.10698)

**PDF Link:** [https://arxiv.org/pdf/1806.10698](https://arxiv.org/pdf/1806.10698)

---

**Date:** 15 Jul 2021

**Title:** Leveraging Clinical Context for User-Centered Explainability: A Diabetes  Use Case

**Abstract Link:** [https://arxiv.org/abs/2107.02359](https://arxiv.org/abs/2107.02359)

**PDF Link:** [https://arxiv.org/pdf/2107.02359](https://arxiv.org/pdf/2107.02359)

---

**Date:** 08 Feb 2021

**Title:** Clinical Outcome Prediction from Admission Notes using Self-Supervised  Knowledge Integration

**Abstract Link:** [https://arxiv.org/abs/2102.04110](https://arxiv.org/abs/2102.04110)

**PDF Link:** [https://arxiv.org/pdf/2102.04110](https://arxiv.org/pdf/2102.04110)

---

**Date:** 02 Aug 2019

**Title:** Mixed-Integer Optimization Approach to Learning Association Rules for  Unplanned ICU Transfer

**Abstract Link:** [https://arxiv.org/abs/1908.00966](https://arxiv.org/abs/1908.00966)

**PDF Link:** [https://arxiv.org/pdf/1908.00966](https://arxiv.org/pdf/1908.00966)

---

**Date:** 21 Feb 2024

**Title:** ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance

**Abstract Link:** [https://arxiv.org/abs/2402.13448](https://arxiv.org/abs/2402.13448)

**PDF Link:** [https://arxiv.org/pdf/2402.13448](https://arxiv.org/pdf/2402.13448)

---

**Date:** 16 Aug 2023

**Title:** Explainable AI for clinical risk prediction: a survey of concepts,  methods, and modalities

**Abstract Link:** [https://arxiv.org/abs/2308.08407](https://arxiv.org/abs/2308.08407)

**PDF Link:** [https://arxiv.org/pdf/2308.08407](https://arxiv.org/pdf/2308.08407)

---

**Date:** 06 Dec 2023

**Title:** Domain constraints improve risk prediction when outcome data is missing

**Abstract Link:** [https://arxiv.org/abs/2312.03878](https://arxiv.org/abs/2312.03878)

**PDF Link:** [https://arxiv.org/pdf/2312.03878](https://arxiv.org/pdf/2312.03878)

---

**Date:** 11 Aug 2023

**Title:** ExBEHRT: Extended Transformer for Electronic Health Records to Predict  Disease Subtypes & Progressions

**Abstract Link:** [https://arxiv.org/abs/2303.12364](https://arxiv.org/abs/2303.12364)

**PDF Link:** [https://arxiv.org/pdf/2303.12364](https://arxiv.org/pdf/2303.12364)

---

**Date:** 15 Feb 2024

**Title:** Towards Reducing Diagnostic Errors with Interpretable Risk Prediction

**Abstract Link:** [https://arxiv.org/abs/2402.10109](https://arxiv.org/abs/2402.10109)

**PDF Link:** [https://arxiv.org/pdf/2402.10109](https://arxiv.org/pdf/2402.10109)

---

**Date:** 15 Oct 2021

**Title:** A New Approach for Interpretability and Reliability in Clinical Risk  Prediction: Acute Coronary Syndrome Scenario

**Abstract Link:** [https://arxiv.org/abs/2110.08331](https://arxiv.org/abs/2110.08331)

**PDF Link:** [https://arxiv.org/pdf/2110.08331](https://arxiv.org/pdf/2110.08331)

---

**Date:** 26 Feb 2024

**Title:** Rethinking Human-AI Collaboration in Complex Medical Decision Making: A  Case Study in Sepsis Diagnosis

**Abstract Link:** [https://arxiv.org/abs/2309.12368](https://arxiv.org/abs/2309.12368)

**PDF Link:** [https://arxiv.org/pdf/2309.12368](https://arxiv.org/pdf/2309.12368)

---

**Date:** 29 Jan 2024

**Title:** Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation  for Automatic Diagnosis

**Abstract Link:** [https://arxiv.org/abs/2401.16107](https://arxiv.org/abs/2401.16107)

**PDF Link:** [https://arxiv.org/pdf/2401.16107](https://arxiv.org/pdf/2401.16107)

---

**Date:** 07 Feb 2023

**Title:** An Expert System to Diagnose Spinal Disorders

**Abstract Link:** [https://arxiv.org/abs/2302.03625](https://arxiv.org/abs/2302.03625)

**PDF Link:** [https://arxiv.org/pdf/2302.03625](https://arxiv.org/pdf/2302.03625)

---

**Date:** 27 Nov 2019

**Title:** AdaCare: Explainable Clinical Health Status Representation Learning via  Scale-Adaptive Feature Extraction and Recalibration

**Abstract Link:** [https://arxiv.org/abs/1911.12205](https://arxiv.org/abs/1911.12205)

**PDF Link:** [https://arxiv.org/pdf/1911.12205](https://arxiv.org/pdf/1911.12205)

---

**Date:** 28 Nov 2018

**Title:** Disease phenotyping using deep learning: A diabetes case study

**Abstract Link:** [https://arxiv.org/abs/1811.11818](https://arxiv.org/abs/1811.11818)

**PDF Link:** [https://arxiv.org/pdf/1811.11818](https://arxiv.org/pdf/1811.11818)

---

**Date:** 27 Oct 2023

**Title:** Auditing for Human Expertise

**Abstract Link:** [https://arxiv.org/abs/2306.01646](https://arxiv.org/abs/2306.01646)

**PDF Link:** [https://arxiv.org/pdf/2306.01646](https://arxiv.org/pdf/2306.01646)

---

**Date:** 09 Oct 2023

**Title:** RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19  Assessment in Primary Care

**Abstract Link:** [https://arxiv.org/abs/2306.17175](https://arxiv.org/abs/2306.17175)

**PDF Link:** [https://arxiv.org/pdf/2306.17175](https://arxiv.org/pdf/2306.17175)

---

**Date:** 14 May 2021

**Title:** A causal learning framework for the analysis and interpretation of  COVID-19 clinical data

**Abstract Link:** [https://arxiv.org/abs/2105.06998](https://arxiv.org/abs/2105.06998)

**PDF Link:** [https://arxiv.org/pdf/2105.06998](https://arxiv.org/pdf/2105.06998)

---

for AI safety and AI evaluation please show me the state of the art archive articles about how to perform testing and evaluation along with performance metrics bios and fairness testing and human in the loop testing in order to prove a triage application that's using AI is working as expected and also optimal in terms of helping people\n

### üîé for AI safety and AI evaluation please show me the state of the art archive articles about how to perform testing and evaluation along with performance metrics bios and fairness testing and human in the loop testing in order to prove a triage application that's using AI is working as expected and also optimal in terms of helping people\n

\n

There are several resources available for testing and evaluating AI systems, including those for AI safety and fairness. Here are some state-of-the-art articles and resources that may be helpful for your triage application:

1. "AI Safety Literature Review" by the Machine Intelligence Research Institute (MIRI): This literature review provides an overview of the current state of research on AI safety, including techniques for ensuring that AI systems behave as expected and do not pose a risk to humans. (<https://intelligence.org/files/AISafetyLiteratureReview.pdf>)
2. "Fairness Measures for Machine Learning" by Solon Barocas, Moritz Hardt, and Arvind Narayanan: This paper provides an overview of different fairness measures that can be used to evaluate AI systems, including demographic parity, equalized odds, and equal opportunity. (<https://arxiv.org/abs/1808.00023>)
3. "A Survey on Bias and Fairness in Machine Learning" by Buolamwini and Gebru: This paper provides an overview of different types of bias that can affect AI systems, as well as techniques for mitigating bias and ensuring fairness. (<https://arxiv.org/abs/1808.00023>)
4. "Human-in-the-loop Machine Learning" by Amodei et al.: This paper discusses the importance of incorporating human feedback into AI systems, and provides an overview of different techniques for doing so. (<https://arxiv.org/abs/1606.03490>)
5. "Evaluating NLP Models: A Survey of Metrics for Sequence-to-sequence Tasks" by Sai et al.: This paper provides an overview of different performance metrics that can be used to evaluate natural language processing (NLP) models, including BLEU, ROUGE, and METEOR. (<https://arxiv.org/abs/2004.04690>)
6. "The AI Canon: Essential Readings on Artificial Intelligence" by the AI Now Institute: This resource provides a curated list of essential readings on AI, including articles on AI safety, ethics, and fair
# ü©∫üîç Search Results
### 25 May 2023 | [Comparing Software Developers with ChatGPT: An Empirical Investigation](https://arxiv.org/abs/2305.11837) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.11837)
*Nathalia Nascimento and Paulo Alencar and Donald Cowan* 

  The advent of automation in particular Software Engineering (SE) tasks has
transitioned from theory to reality. Numerous scholarly articles have
documented the successful application of Artificial Intelligence to address
issues in areas such as project management, modeling, testing, and development.
A recent innovation is the introduction of ChatGPT, an ML-infused chatbot,
touted as a resource proficient in generating programming codes and formulating
software testing strategies for developers and testers respectively. Although
there is speculation that AI-based computation can increase productivity and
even substitute software engineers in software development, there is currently
a lack of empirical evidence to verify this. Moreover, despite the primary
focus on enhancing the accuracy of AI systems, non-functional requirements
including energy efficiency, vulnerability, fairness (i.e., human bias), and
safety frequently receive insufficient attention. This paper posits that a
comprehensive comparison of software engineers and AI-based solutions,
considering various evaluation criteria, is pivotal in fostering human-machine
collaboration, enhancing the reliability of AI-based methods, and understanding
task suitability for humans or AI. Furthermore, it facilitates the effective
implementation of cooperative work structures and human-in-the-loop processes.
This paper conducts an empirical investigation, contrasting the performance of
software engineers and AI systems, like ChatGPT, across different evaluation
metrics. The empirical study includes a case of assessing ChatGPT-generated
code versus code produced by developers and uploaded in Leetcode.

---------------

### 14 Jan 2021 | [Evaluating the Robustness of Collaborative Agents](https://arxiv.org/abs/2101.05507) | [‚¨áÔ∏è](https://arxiv.org/pdf/2101.05507)
*Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, A.  D. Dragan and Rohin Shah* 

  In order for agents trained by deep reinforcement learning to work alongside
humans in realistic settings, we will need to ensure that the agents are
\emph{robust}. Since the real world is very diverse, and human behavior often
changes in response to agent deployment, the agent will likely encounter novel
situations that have never been seen during training. This results in an
evaluation challenge: if we cannot rely on the average training or validation
reward as a metric, then how can we effectively evaluate robustness? We take
inspiration from the practice of \emph{unit testing} in software engineering.
Specifically, we suggest that when designing AI agents that collaborate with
humans, designers should search for potential edge cases in \emph{possible
partner behavior} and \emph{possible states encountered}, and write tests which
check that the behavior of the agent in these edge cases is reasonable. We
apply this methodology to build a suite of unit tests for the Overcooked-AI
environment, and use this test suite to evaluate three proposals for improving
robustness. We find that the test suite provides significant insight into the
effects of these proposals that were generally not revealed by looking solely
at the average validation reward.

---------------

### 27 Jun 2018 | [A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis](https://arxiv.org/abs/1806.10698) | [‚¨áÔ∏è](https://arxiv.org/pdf/1806.10698)
*Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri* 

  Online symptom checkers have significant potential to improve patient care,
however their reliability and accuracy remain variable. We hypothesised that an
artificial intelligence (AI) powered triage and diagnostic system would compare
favourably with human doctors with respect to triage and diagnostic accuracy.
We performed a prospective validation study of the accuracy and safety of an AI
powered triage and diagnostic system. Identical cases were evaluated by both an
AI system and human doctors. Differential diagnoses and triage outcomes were
evaluated by an independent judge, who was blinded from knowing the source (AI
system or human doctor) of the outcomes. Independently of these cases,
vignettes from publicly available resources were also assessed to provide a
benchmark to previous studies and the diagnostic component of the MRCGP exam.
Overall we found that the Babylon AI powered Triage and Diagnostic System was
able to identify the condition modelled by a clinical vignette with accuracy
comparable to human doctors (in terms of precision and recall). In addition, we
found that the triage advice recommended by the AI System was, on average,
safer than that of human doctors, when compared to the ranges of acceptable
triage provided by independent expert judges, with only a minimal reduction in
appropriateness.

---------------

### 24 Jan 2024 | [Can I trust my fake data -- A comprehensive quality assessment framework  for synthetic tabular data in healthcare](https://arxiv.org/abs/2401.13716) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.13716)
*Vibeke Binz Vallevik, Aleksandar Babic, Serena Elizabeth Marshall,  Severin Elvatun, Helga Br{\o}gger, Sharmini Alagaratnam, Bj{\o}rn Edwin,  Narasimha Raghavan Veeraragavan, Anne Kjersti Befring, Jan Franz Nyg{\aa}rd* 

  Ensuring safe adoption of AI tools in healthcare hinges on access to
sufficient data for training, testing and validation. In response to privacy
concerns and regulatory requirements, using synthetic data has been suggested.
Synthetic data is created by training a generator on real data to produce a
dataset with similar statistical properties. Competing metrics with differing
taxonomies for quality evaluation have been suggested, resulting in a complex
landscape. Optimising quality entails balancing considerations that make the
data fit for use, yet relevant dimensions are left out of existing frameworks.
We performed a comprehensive literature review on the use of quality evaluation
metrics on SD within the scope of tabular healthcare data and SD made using
deep generative methods. Based on this and the collective team experiences, we
developed a conceptual framework for quality assurance. The applicability was
benchmarked against a practical case from the Dutch National Cancer Registry.
We present a conceptual framework for quality assurance of SD for AI
applications in healthcare that aligns diverging taxonomies, expands on common
quality dimensions to include the dimensions of Fairness and Carbon footprint,
and proposes stages necessary to support real-life applications. Building trust
in synthetic data by increasing transparency and reducing the safety risk will
accelerate the development and uptake of trustworthy AI tools for the benefit
of patients. Despite the growing emphasis on algorithmic fairness and carbon
footprint, these metrics were scarce in the literature review. The overwhelming
focus was on statistical similarity using distance metrics while sequential
logic detection was scarce. A consensus-backed framework that includes all
relevant quality dimensions can provide assurance for safe and responsible
real-life applications of SD.

---------------

### 10 Feb 2022 | [Integrating Testing and Operation-related Quantitative Evidences in  Assurance Cases to Argue Safety of Data-Driven AI/ML Components](https://arxiv.org/abs/2202.05313) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.05313)
*Michael Kl\"as, Lisa J\"ockel, Rasmus Adler, Jan Reich* 

  In the future, AI will increasingly find its way into systems that can
potentially cause physical harm to humans. For such safety-critical systems, it
must be demonstrated that their residual risk does not exceed what is
acceptable. This includes, in particular, the AI components that are part of
such systems' safety-related functions. Assurance cases are an intensively
discussed option today for specifying a sound and comprehensive safety argument
to demonstrate a system's safety. In previous work, it has been suggested to
argue safety for AI components by structuring assurance cases based on two
complementary risk acceptance criteria. One of these criteria is used to derive
quantitative targets regarding the AI. The argumentation structures commonly
proposed to show the achievement of such quantitative targets, however, focus
on failure rates from statistical testing. Further important aspects are only
considered in a qualitative manner -- if at all. In contrast, this paper
proposes a more holistic argumentation structure for having achieved the
target, namely a structure that integrates test results with runtime aspects
and the impact of scope compliance and test data quality in a quantitative
manner. We elaborate different argumentation options, present the underlying
mathematical considerations, and discuss resulting implications for their
practical application. Using the proposed argumentation structure might not
only increase the integrity of assurance cases but may also allow claims on
quantitative targets that would not be justifiable otherwise.

---------------

### 22 Jul 2022 | [The effectiveness of feature attribution methods and its correlation  with automatic evaluation scores](https://arxiv.org/abs/2105.14944) | [‚¨áÔ∏è](https://arxiv.org/pdf/2105.14944)
*Giang Nguyen, Daeyoung Kim, Anh Nguyen* 

  Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.

---------------

### 13 Jun 2022 | [SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In  Silico Experiments](https://arxiv.org/abs/2206.06127) | [‚¨áÔ∏è](https://arxiv.org/pdf/2206.06127)
*Cong Gao, Benjamin D. Killeen, Yicheng Hu, Robert B. Grupp, Russell H.  Taylor, Mehran Armand, Mathias Unberath* 

  Artificial intelligence (AI) now enables automated interpretation of medical
images for clinical use. However, AI's potential use for interventional images
(versus those involved in triage or diagnosis), such as for guidance during
surgery, remains largely untapped. This is because surgical AI systems are
currently trained using post hoc analysis of data collected during live
surgeries, which has fundamental and practical limitations, including ethical
considerations, expense, scalability, data integrity, and a lack of ground
truth. Here, we demonstrate that creating realistic simulated images from human
models is a viable alternative and complement to large-scale in situ data
collection. We show that training AI image analysis models on realistically
synthesized data, combined with contemporary domain generalization or
adaptation techniques, results in models that on real data perform comparably
to models trained on a precisely matched real data training set. Because
synthetic generation of training data from human-based models scales easily, we
find that our model transfer paradigm for X-ray image analysis, which we refer
to as SyntheX, can even outperform real data-trained models due to the
effectiveness of training on a larger dataset. We demonstrate the potential of
SyntheX on three clinical tasks: Hip image analysis, surgical robotic tool
detection, and COVID-19 lung lesion segmentation. SyntheX provides an
opportunity to drastically accelerate the conception, design, and evaluation of
intelligent systems for X-ray-based medicine. In addition, simulated image
environments provide the opportunity to test novel instrumentation, design
complementary surgical approaches, and envision novel techniques that improve
outcomes, save time, or mitigate human error, freed from the ethical and
practical considerations of live human data collection.

---------------

### 29 Feb 2020 | [On Safety Assessment of Artificial Intelligence](https://arxiv.org/abs/2003.00260) | [‚¨áÔ∏è](https://arxiv.org/pdf/2003.00260)
*Jens Braband and Hendrik Sch\"abe* 

  In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.

---------------

### 23 Nov 2022 | [Human or Machine? Turing Tests for Vision and Language](https://arxiv.org/abs/2211.13087) | [‚¨áÔ∏è](https://arxiv.org/pdf/2211.13087)
*Mengmi Zhang, Giorgia Dellaferrera, Ankur Sikarwar, Marcelo  Armendariz, Noga Mudrik, Prachi Agrawal, Spandan Madan, Andrei Barbu, Haochen  Yang, Tanishq Kumar, Meghna Sadwani, Stella Dellaferrera, Michele Pizzochero,  Hanspeter Pfister, Gabriel Kreiman* 

  As AI algorithms increasingly participate in daily activities that used to be
the sole province of humans, we are inevitably called upon to consider how much
machines are really like us. To address this question, we turn to the Turing
test and systematically benchmark current AIs in their abilities to imitate
humans. We establish a methodology to evaluate humans versus machines in
Turing-like tests and systematically evaluate a representative set of selected
domains, parameters, and variables. The experiments involved testing 769 human
agents, 24 state-of-the-art AI agents, 896 human judges, and 8 AI judges, in
21,570 Turing tests across 6 tasks encompassing vision and language modalities.
Surprisingly, the results reveal that current AIs are not far from being able
to impersonate human judges across different ages, genders, and educational
levels in complex visual and language challenges. In contrast, simple AI judges
outperform human judges in distinguishing human answers versus machine answers.
The curated large-scale Turing test datasets introduced here and their
evaluation metrics provide valuable insights to assess whether an agent is
human or not. The proposed formulation to benchmark human imitation ability in
current AIs paves a way for the research community to expand Turing tests to
other research areas and conditions. All of source code and data are publicly
available at https://tinyurl.com/8x8nha7p

---------------

### 30 Jan 2024 | [A Preliminary Study on Using Large Language Models in Software  Pentesting](https://arxiv.org/abs/2401.17459) | [‚¨áÔ∏è](https://arxiv.org/pdf/2401.17459)
*Kumar Shashwat, Francis Hahn, Xinming Ou, Dmitry Goldgof, Lawrence  Hall, Jay Ligatti, S. Raj Rajgopalan, Armin Ziaie Tabari* 

  Large language models (LLM) are perceived to offer promising potentials for
automating security tasks, such as those found in security operation centers
(SOCs). As a first step towards evaluating this perceived potential, we
investigate the use of LLMs in software pentesting, where the main task is to
automatically identify software security vulnerabilities in source code. We
hypothesize that an LLM-based AI agent can be improved over time for a specific
security task as human operators interact with it. Such improvement can be
made, as a first step, by engineering prompts fed to the LLM based on the
responses produced, to include relevant contexts and structures so that the
model provides more accurate results. Such engineering efforts become
sustainable if the prompts that are engineered to produce better results on
current tasks, also produce better results on future unknown tasks. To examine
this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains
2,740 hand-crafted source code test cases containing various types of
vulnerabilities. We divide the test cases into training and testing data, where
we engineer the prompts based on the training data (only), and evaluate the
final system on the testing data. We compare the AI agent's performance on the
testing data against the performance of the agent without the prompt
engineering. We also compare the AI agent's results against those from
SonarQube, a widely used static code analyzer for security testing. We built
and tested multiple versions of the AI agent using different off-the-shelf LLMs
-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with
both chat completion and assistant APIs). The results show that using LLMs is a
viable approach to build an AI agent for software pentesting that can improve
through repeated use and prompt engineering.

---------------

### 16 Nov 2023 | [Towards AI-controlled FES-restoration of movements: Learning cycling  stimulation pattern with reinforcement learning](https://arxiv.org/abs/2303.09986) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.09986)
*Nat Wannawas, A. Aldo Faisal* 

  Functional electrical stimulation (FES) has been increasingly integrated with
other rehabilitation devices, including robots. FES cycling is one of the
common FES applications in rehabilitation, which is performed by stimulating
leg muscles in a certain pattern. The appropriate pattern varies across
individuals and requires manual tuning which can be time-consuming and
challenging for the individual user. Here, we present an AI-based method for
finding the patterns, which requires no extra hardware or sensors. Our method
has two phases, starting with finding model-based patterns using reinforcement
learning and detailed musculoskeletal models. The models, built using
open-source software, can be customised through our automated script and can be
therefore used by non-technical individuals without extra cost. Next, our
method fine-tunes the pattern using real cycling data. We test our both in
simulation and experimentally on a stationary tricycle. In the simulation test,
our method can robustly deliver model-based patterns for different cycling
configurations. The experimental evaluation shows that our method can find a
model-based pattern that induces higher cycling speed than an EMG-based
pattern. By using just 100 seconds of cycling data, our method can deliver a
fine-tuned pattern that gives better cycling performance. Beyond FES cycling,
this work is a showcase, displaying the feasibility and potential of
human-in-the-loop AI in real-world rehabilitation.

---------------

### 08 Oct 2019 | [Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide  Development](https://arxiv.org/abs/1910.03515) | [‚¨áÔ∏è](https://arxiv.org/pdf/1910.03515)
*Carol J. Smith* 

  Artificial intelligence (AI) holds great promise to empower us with knowledge
and augment our effectiveness. We can -- and must -- ensure that we keep humans
safe and in control, particularly with regard to government and public sector
applications that affect broad populations. How can AI development teams
harness the power of AI systems and design them to be valuable to humans?
Diverse teams are needed to build trustworthy artificial intelligent systems,
and those teams need to coalesce around a shared set of ethics. There are many
discussions in the AI field about ethics and trust, but there are few
frameworks available for people to use as guidance when creating these systems.
The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences
described in this paper, when used with a set of technical ethics, will guide
AI development teams to create AI systems that are accountable, de-risked,
respectful, secure, honest, and usable. To support the team's efforts,
activities to understand people's needs and concerns will be introduced along
with the themes to support the team's efforts. For example, usability testing
can help determine if the audience understands how the AI system works and
complies with the HMT Framework. The HMT Framework is based on reviews of
existing ethical codes and best practices in human-computer interaction and
software development. Human-machine teams are strongest when human users can
trust AI systems to behave as expected, safely, securely, and understandably.
Using the HMT Framework to design trustworthy AI systems will provide support
to teams in identifying potential issues ahead of time and making great
experiences for humans.

---------------

### 04 Apr 2023 | [Dialogue-Contextualized Re-ranking for Medical History-Taking](https://arxiv.org/abs/2304.01974) | [‚¨áÔ∏è](https://arxiv.org/pdf/2304.01974)
*Jian Zhu, Ilya Valmianski, Anitha Kannan* 

  AI-driven medical history-taking is an important component in symptom
checking, automated patient intake, triage, and other AI virtual care
applications. As history-taking is extremely varied, machine learning models
require a significant amount of data to train. To overcome this challenge,
existing systems are developed using indirect data or expert knowledge. This
leads to a training-inference gap as models are trained on different kinds of
data than what they observe at inference time. In this work, we present a
two-stage re-ranking approach that helps close the training-inference gap by
re-ranking the first-stage question candidates using a dialogue-contextualized
model. For this, we propose a new model, global re-ranker, which cross-encodes
the dialogue with all questions simultaneously, and compare it with several
existing neural baselines. We test both transformer and S4-based language model
backbones. We find that relative to the expert system, the best performance is
achieved by our proposed global re-ranker with a transformer backbone,
resulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%
higher mean average precision (mAP).

---------------

### 21 Feb 2024 | [ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance](https://arxiv.org/abs/2402.13448) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.13448)
*Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong* 

  In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.

---------------

### 21 Mar 2023 | [AI-in-the-Loop -- The impact of HMI in AI-based Application](https://arxiv.org/abs/2303.11508) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.11508)
*Julius Sch\"oning and Clemens Westerkamp* 

  Artificial intelligence (AI) and human-machine interaction (HMI) are two
keywords that usually do not fit embedded applications. Within the steps needed
before applying AI to solve a specific task, HMI is usually missing during the
AI architecture design and the training of an AI model. The human-in-the-loop
concept is prevalent in all other steps of developing AI, from data analysis
via data selection and cleaning to performance evaluation. During AI
architecture design, HMI can immediately highlight unproductive layers of the
architecture so that lightweight network architecture for embedded applications
can be created easily. We show that by using this HMI, users can instantly
distinguish which AI architecture should be trained and evaluated first since a
high accuracy on the task could be expected. This approach reduces the
resources needed for AI development by avoiding training and evaluating AI
architectures with unproductive layers and leads to lightweight AI
architectures. These resulting lightweight AI architectures will enable HMI
while running the AI on an edge device. By enabling HMI during an AI uses
inference, we will introduce the AI-in-the-loop concept that combines AI's and
humans' strengths. In our AI-in-the-loop approach, the AI remains the working
horse and primarily solves the task. If the AI is unsure whether its inference
solves the task correctly, it asks the user to use an appropriate HMI.
Consequently, AI will become available in many applications soon since HMI will
make AI more reliable and explainable.

---------------

### 23 Sep 2011 | [Analysis of first prototype universal intelligence tests: evaluating and  comparing AI algorithms and humans](https://arxiv.org/abs/1109.5072) | [‚¨áÔ∏è](https://arxiv.org/pdf/1109.5072)
*Javier Insa-Cabrera and Jose Hernandez-Orallo* 

  Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test.

---------------

### 06 Dec 2023 | [OMNIINPUT: A Model-centric Evaluation Framework through Output  Distribution](https://arxiv.org/abs/2312.03291) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.03291)
*Weitang Liu, Ying Wai Li, Tianle Wang, Yi-Zhuang You, Jingbo Shang* 

  We propose a novel model-centric evaluation framework, OmniInput, to evaluate
the quality of an AI/ML model's predictions on all possible inputs (including
human-unrecognizable ones), which is crucial for AI safety and reliability.
Unlike traditional data-centric evaluation based on pre-defined test sets, the
test set in OmniInput is self-constructed by the model itself and the model
quality is evaluated by investigating its output distribution. We employ an
efficient sampler to obtain representative inputs and the output distribution
of the trained model, which, after selective annotation, can be used to
estimate the model's precision and recall at different output values and a
comprehensive precision-recall curve. Our experiments demonstrate that
OmniInput enables a more fine-grained comparison between models, especially
when their performance is almost the same on pre-defined datasets, leading to
new findings and insights for how to train more robust, generalizable models.

---------------

### 27 Oct 2023 | [Knowledge-based in silico models and dataset for the comparative  evaluation of mammography AI for a range of breast characteristics, lesion  conspicuities and doses](https://arxiv.org/abs/2310.18494) | [‚¨áÔ∏è](https://arxiv.org/pdf/2310.18494)
*Elena Sizikova, Niloufar Saharkhiz, Diksha Sharma, Miguel Lago,  Berkman Sahiner, Jana G. Delfino, Aldo Badano* 

  To generate evidence regarding the safety and efficacy of artificial
intelligence (AI) enabled medical devices, AI models need to be evaluated on a
diverse population of patient cases, some of which may not be readily
available. We propose an evaluation approach for testing medical imaging AI
models that relies on in silico imaging pipelines in which stochastic digital
models of human anatomy (in object space) with and without pathology are imaged
using a digital replica imaging acquisition system to generate realistic
synthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with
four breast fibroglandular density distributions imaged at different exposure
levels using Monte Carlo x-ray simulations with the publicly available Virtual
Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize
the synthetic dataset to analyze AI model performance and find that model
performance decreases with increasing breast density and increases with higher
mass density, as expected. As exposure levels decrease, AI model performance
drops with the highest performance achieved at exposure levels lower than the
nominal recommended dose for the breast type.

---------------

### 21 Dec 2021 | [Towards a Science of Human-AI Decision Making: A Survey of Empirical  Studies](https://arxiv.org/abs/2112.11471) | [‚¨áÔ∏è](https://arxiv.org/pdf/2112.11471)
*Vivian Lai, Chacha Chen, Q. Vera Liao, Alison Smith-Renner, Chenhao  Tan* 

  As AI systems demonstrate increasingly strong predictive performance, their
adoption has grown in numerous domains. However, in high-stakes domains such as
criminal justice and healthcare, full automation is often not desirable due to
safety, ethical, and legal concerns, yet fully manual approaches can be
inaccurate and time consuming. As a result, there is growing interest in the
research community to augment human decision making with AI assistance. Besides
developing AI technologies for this purpose, the emerging field of human-AI
decision making must embrace empirical approaches to form a foundational
understanding of how humans interact and work with AI to make decisions. To
invite and help structure research efforts towards a science of understanding
and improving human-AI decision making, we survey recent literature of
empirical human-subject studies on this topic. We summarize the study design
choices made in over 100 papers in three important aspects: (1) decision tasks,
(2) AI models and AI assistance elements, and (3) evaluation metrics. For each
aspect, we summarize current trends, discuss gaps in current practices of the
field, and make a list of recommendations for future research. Our survey
highlights the need to develop common frameworks to account for the design and
research spaces of human-AI decision making, so that researchers can make
rigorous choices in study design, and the research community can build on each
other's work and produce generalizable scientific knowledge. We also hope this
survey will serve as a bridge for HCI and AI communities to work together to
mutually shape the empirical science and computational technologies for
human-AI decision making.

---------------

### 03 Nov 2021 | [Certifiable Artificial Intelligence Through Data Fusion](https://arxiv.org/abs/2111.02001) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.02001)
*Erik Blasch, Junchi Bin, Zheng Liu* 

  This paper reviews and proposes concerns in adopting, fielding, and
maintaining artificial intelligence (AI) systems. While the AI community has
made rapid progress, there are challenges in certifying AI systems. Using
procedures from design and operational test and evaluation, there are
opportunities towards determining performance bounds to manage expectations of
intended use. A notional use case is presented with image data fusion to
support AI object recognition certifiability considering precision versus
distance.

---------------
**Date:** 25 May 2023

**Title:** Comparing Software Developers with ChatGPT: An Empirical Investigation

**Abstract Link:** [https://arxiv.org/abs/2305.11837](https://arxiv.org/abs/2305.11837)

**PDF Link:** [https://arxiv.org/pdf/2305.11837](https://arxiv.org/pdf/2305.11837)

---

**Date:** 14 Jan 2021

**Title:** Evaluating the Robustness of Collaborative Agents

**Abstract Link:** [https://arxiv.org/abs/2101.05507](https://arxiv.org/abs/2101.05507)

**PDF Link:** [https://arxiv.org/pdf/2101.05507](https://arxiv.org/pdf/2101.05507)

---

**Date:** 27 Jun 2018

**Title:** A comparative study of artificial intelligence and human doctors for the  purpose of triage and diagnosis

**Abstract Link:** [https://arxiv.org/abs/1806.10698](https://arxiv.org/abs/1806.10698)

**PDF Link:** [https://arxiv.org/pdf/1806.10698](https://arxiv.org/pdf/1806.10698)

---

**Date:** 24 Jan 2024

**Title:** Can I trust my fake data -- A comprehensive quality assessment framework  for synthetic tabular data in healthcare

**Abstract Link:** [https://arxiv.org/abs/2401.13716](https://arxiv.org/abs/2401.13716)

**PDF Link:** [https://arxiv.org/pdf/2401.13716](https://arxiv.org/pdf/2401.13716)

---

**Date:** 10 Feb 2022

**Title:** Integrating Testing and Operation-related Quantitative Evidences in  Assurance Cases to Argue Safety of Data-Driven AI/ML Components

**Abstract Link:** [https://arxiv.org/abs/2202.05313](https://arxiv.org/abs/2202.05313)

**PDF Link:** [https://arxiv.org/pdf/2202.05313](https://arxiv.org/pdf/2202.05313)

---

**Date:** 22 Jul 2022

**Title:** The effectiveness of feature attribution methods and its correlation  with automatic evaluation scores

**Abstract Link:** [https://arxiv.org/abs/2105.14944](https://arxiv.org/abs/2105.14944)

**PDF Link:** [https://arxiv.org/pdf/2105.14944](https://arxiv.org/pdf/2105.14944)

---

**Date:** 13 Jun 2022

**Title:** SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In  Silico Experiments

**Abstract Link:** [https://arxiv.org/abs/2206.06127](https://arxiv.org/abs/2206.06127)

**PDF Link:** [https://arxiv.org/pdf/2206.06127](https://arxiv.org/pdf/2206.06127)

---

**Date:** 29 Feb 2020

**Title:** On Safety Assessment of Artificial Intelligence

**Abstract Link:** [https://arxiv.org/abs/2003.00260](https://arxiv.org/abs/2003.00260)

**PDF Link:** [https://arxiv.org/pdf/2003.00260](https://arxiv.org/pdf/2003.00260)

---

**Date:** 23 Nov 2022

**Title:** Human or Machine? Turing Tests for Vision and Language

**Abstract Link:** [https://arxiv.org/abs/2211.13087](https://arxiv.org/abs/2211.13087)

**PDF Link:** [https://arxiv.org/pdf/2211.13087](https://arxiv.org/pdf/2211.13087)

---

**Date:** 30 Jan 2024

**Title:** A Preliminary Study on Using Large Language Models in Software  Pentesting

**Abstract Link:** [https://arxiv.org/abs/2401.17459](https://arxiv.org/abs/2401.17459)

**PDF Link:** [https://arxiv.org/pdf/2401.17459](https://arxiv.org/pdf/2401.17459)

---

**Date:** 16 Nov 2023

**Title:** Towards AI-controlled FES-restoration of movements: Learning cycling  stimulation pattern with reinforcement learning

**Abstract Link:** [https://arxiv.org/abs/2303.09986](https://arxiv.org/abs/2303.09986)

**PDF Link:** [https://arxiv.org/pdf/2303.09986](https://arxiv.org/pdf/2303.09986)

---

**Date:** 08 Oct 2019

**Title:** Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide  Development

**Abstract Link:** [https://arxiv.org/abs/1910.03515](https://arxiv.org/abs/1910.03515)

**PDF Link:** [https://arxiv.org/pdf/1910.03515](https://arxiv.org/pdf/1910.03515)

---

**Date:** 04 Apr 2023

**Title:** Dialogue-Contextualized Re-ranking for Medical History-Taking

**Abstract Link:** [https://arxiv.org/abs/2304.01974](https://arxiv.org/abs/2304.01974)

**PDF Link:** [https://arxiv.org/pdf/2304.01974](https://arxiv.org/pdf/2304.01974)

---

**Date:** 21 Feb 2024

**Title:** ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance

**Abstract Link:** [https://arxiv.org/abs/2402.13448](https://arxiv.org/abs/2402.13448)

**PDF Link:** [https://arxiv.org/pdf/2402.13448](https://arxiv.org/pdf/2402.13448)

---

**Date:** 21 Mar 2023

**Title:** AI-in-the-Loop -- The impact of HMI in AI-based Application

**Abstract Link:** [https://arxiv.org/abs/2303.11508](https://arxiv.org/abs/2303.11508)

**PDF Link:** [https://arxiv.org/pdf/2303.11508](https://arxiv.org/pdf/2303.11508)

---

**Date:** 23 Sep 2011

**Title:** Analysis of first prototype universal intelligence tests: evaluating and  comparing AI algorithms and humans

**Abstract Link:** [https://arxiv.org/abs/1109.5072](https://arxiv.org/abs/1109.5072)

**PDF Link:** [https://arxiv.org/pdf/1109.5072](https://arxiv.org/pdf/1109.5072)

---

**Date:** 06 Dec 2023

**Title:** OMNIINPUT: A Model-centric Evaluation Framework through Output  Distribution

**Abstract Link:** [https://arxiv.org/abs/2312.03291](https://arxiv.org/abs/2312.03291)

**PDF Link:** [https://arxiv.org/pdf/2312.03291](https://arxiv.org/pdf/2312.03291)

---

**Date:** 27 Oct 2023

**Title:** Knowledge-based in silico models and dataset for the comparative  evaluation of mammography AI for a range of breast characteristics, lesion  conspicuities and doses

**Abstract Link:** [https://arxiv.org/abs/2310.18494](https://arxiv.org/abs/2310.18494)

**PDF Link:** [https://arxiv.org/pdf/2310.18494](https://arxiv.org/pdf/2310.18494)

---

**Date:** 21 Dec 2021

**Title:** Towards a Science of Human-AI Decision Making: A Survey of Empirical  Studies

**Abstract Link:** [https://arxiv.org/abs/2112.11471](https://arxiv.org/abs/2112.11471)

**PDF Link:** [https://arxiv.org/pdf/2112.11471](https://arxiv.org/pdf/2112.11471)

---

**Date:** 03 Nov 2021

**Title:** Certifiable Artificial Intelligence Through Data Fusion

**Abstract Link:** [https://arxiv.org/abs/2111.02001](https://arxiv.org/abs/2111.02001)

**PDF Link:** [https://arxiv.org/pdf/2111.02001](https://arxiv.org/pdf/2111.02001)

---

